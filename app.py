import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import joblib
import os

from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from matplotlib.ticker import FuncFormatter


# =========================
# CONFIG
# =========================
st.set_page_config(page_title="·ª®ng d·ª•ng D·ª± ƒëo√°n Gi√° Laptop", layout="wide")


# =========================
# FUNCTIONS
# =========================
def load_data(train_file, val_file, test_file):
    df_train = pd.read_csv(train_file)
    df_val = pd.read_csv(val_file)
    df_test = pd.read_csv(test_file)
    return df_train, df_val, df_test


def align_columns(df_train, df_val, df_test, target="price_base"):
    # Drop title n·∫øu c√≥
    def _drop_title(df):
        if "title" in df.columns:
            return df.drop(columns=["title"])
        return df

    df_train = _drop_title(df_train)
    df_val = _drop_title(df_val)
    df_test = _drop_title(df_test)

    # intersection columns (tr√°nh test th·ª´a c·ªôt)
    common_cols = list(set(df_train.columns) & set(df_val.columns) & set(df_test.columns))

    if target not in df_train.columns or target not in df_val.columns or target not in df_test.columns:
        raise ValueError(f"Thi·∫øu c·ªôt target '{target}' trong 1 trong 3 t·∫≠p d·ªØ li·ªáu.")

    if target in common_cols:
        common_cols.remove(target)

    common_cols.sort()

    X_train = df_train[common_cols].copy()
    y_train = df_train[target].copy()

    X_val = df_val[common_cols].copy()
    y_val = df_val[target].copy()

    X_test = df_test[common_cols].copy()
    y_test = df_test[target].copy()

    # √âp to√†n b·ªô feature sang numeric ƒë·ªÉ XGB/LGBM kh√¥ng b·ªã crash n·∫øu c√≥ c·ªôt object
    # (n·∫øu l√† object -> NaN -> imputer median s·∫Ω x·ª≠ l√Ω)
    for c in common_cols:
        X_train[c] = pd.to_numeric(X_train[c], errors="coerce")
        X_val[c] = pd.to_numeric(X_val[c], errors="coerce")
        X_test[c] = pd.to_numeric(X_test[c], errors="coerce")

    return X_train, y_train, X_val, y_val, X_test, y_test, common_cols


def calculate_metrics(y_true, y_pred):
    r2 = float(r2_score(y_true, y_pred))
    mae = float(mean_absolute_error(y_true, y_pred))
    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))

    # Safe MAPE (tr√°nh chia 0 ho·∫∑c qu√° nh·ªè)
    eps = 1e-8
    denom = np.maximum(np.abs(y_true), eps)
    mape = float(np.mean(np.abs((y_true - y_pred) / denom)) * 100)

    return {"R2": r2, "MAE": mae, "RMSE": rmse, "MAPE (%)": mape}


def build_model(model_type, params):
    """
    Tr·∫£ v·ªÅ pipeline + c·ªù early_stop (ƒë√∫ng flow b·∫°n ƒëang d√πng)
    """
    if model_type == "Random Forest":
        base_model = RandomForestRegressor(
            n_estimators=int(params["n_estimators"]),
            max_depth=None if int(params["max_depth"]) == 0 else int(params["max_depth"]),
            min_samples_split=int(params["min_samples_split"]),
            min_samples_leaf=int(params["min_samples_leaf"]),
            max_features=float(params["max_features"]),
            n_jobs=-1,
            random_state=42
        )
        use_early_stop = False

    elif model_type == "XGBoost":
        # Default t·ªëi ∆∞u h∆°n (g·∫ßn ki·ªÉu notebook hay d√πng ƒë·ªÉ l√™n R2)
        base_model = XGBRegressor(
            n_estimators=int(params["n_estimators"]),
            max_depth=int(params["max_depth"]),
            learning_rate=float(params["learning_rate"]),
            subsample=float(params["subsample"]),
            colsample_bytree=float(params["colsample_bytree"]),
            reg_alpha=float(params["reg_alpha"]),
            reg_lambda=float(params["reg_lambda"]),
            min_child_weight=float(params["min_child_weight"]),
            gamma=float(params["gamma"]),
            objective="reg:squarederror",
            eval_metric="rmse",
            tree_method="hist",   # nhanh v√† ·ªïn tr√™n cloud
            n_jobs=-1,
            random_state=42
        )
        use_early_stop = True

    else:  # LightGBM
        base_model = LGBMRegressor(
            n_estimators=int(params["n_estimators"]),
            max_depth=-1 if int(params["max_depth"]) == 0 else int(params["max_depth"]),
            learning_rate=float(params["learning_rate"]),
            num_leaves=int(params["num_leaves"]),
            subsample=float(params["subsample"]),
            colsample_bytree=float(params["colsample_bytree"]),
            reg_alpha=float(params["reg_alpha"]),
            reg_lambda=float(params["reg_lambda"]),
            min_child_samples=int(params["min_child_samples"]),
            random_state=42,
            n_jobs=-1
        )
        use_early_stop = True

    pipeline = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("model", base_model)
    ])
    return pipeline, use_early_stop


def train_and_eval(model_pipeline, use_early_stop, model_type, X_tr, y_tr, X_vl, y_vl, early_rounds=50):
    """
    FIX XGBoost: ch·∫°y ƒë∆∞·ª£c c·∫£ khi version xgboost tr√™n Streamlit Cloud kh√°c nhau.
    - ∆Øu ti√™n early_stopping_rounds
    - N·∫øu TypeError -> fallback callbacks
    - N·∫øu v·∫´n fail -> fit b√¨nh th∆∞·ªùng (kh√¥ng crash)
    """
    if use_early_stop:
        # Fit imputer tr√™n train -> transform val (ch·ªëng r√≤ r·ªâ)
        X_tr_imp = model_pipeline.named_steps["imputer"].fit_transform(X_tr)
        X_vl_imp = model_pipeline.named_steps["imputer"].transform(X_vl)
        model = model_pipeline.named_steps["model"]

        if model_type == "XGBoost":
            try:
                model.fit(
                    X_tr_imp, y_tr,
                    eval_set=[(X_vl_imp, y_vl)],
                    verbose=False,
                    early_stopping_rounds=int(early_rounds)
                )
            except TypeError:
                # M·ªôt s·ªë b·∫£n xgboost thay ƒë·ªïi API -> d√πng callback
                try:
                    from xgboost.callback import EarlyStopping
                    cb = EarlyStopping(rounds=int(early_rounds), save_best=True, maximize=False)
                    model.fit(
                        X_tr_imp, y_tr,
                        eval_set=[(X_vl_imp, y_vl)],
                        verbose=False,
                        callbacks=[cb]
                    )
                except Exception:
                    # fallback cu·ªëi: train b√¨nh th∆∞·ªùng ƒë·ªÉ app kh√¥ng ch·∫øt
                    model.fit(X_tr_imp, y_tr)

            y_pred_vl = model.predict(X_vl_imp)
            return model_pipeline, y_pred_vl

        # LightGBM
        try:
            model.fit(
                X_tr_imp, y_tr,
                eval_set=[(X_vl_imp, y_vl)],
                eval_metric="l2",
            )
        except TypeError:
            # c√≥ m√¥i tr∆∞·ªùng b·ªã kh√°c ch·ªØ k√Ω h√†m
            model.fit(X_tr_imp, y_tr)

        y_pred_vl = model.predict(X_vl_imp)
        return model_pipeline, y_pred_vl

    # RandomForest
    model_pipeline.fit(X_tr, y_tr)
    y_pred_vl = model_pipeline.predict(X_vl)
    return model_pipeline, y_pred_vl


def _plain_int_formatter():
    # hi·ªÉn th·ªã 15690000 thay v√¨ 1.569e7
    return FuncFormatter(lambda x, pos: f"{int(x):d}")


def plot_scatter(y_true, y_pred):
    fig, ax = plt.subplots(figsize=(6.0, 4.0))
    ax.scatter(y_true, y_pred, alpha=0.5)

    mn = float(min(np.min(y_true), np.min(y_pred)))
    mx = float(max(np.max(y_true), np.max(y_pred)))
    ax.plot([mn, mx], [mn, mx], "r--")

    ax.set_title("Gi√° th·∫≠t vs Gi√° d·ª± ƒëo√°n (Validation)")
    ax.set_xlabel("Gi√° th·∫≠t (VND)")
    ax.set_ylabel("Gi√° d·ª± ƒëo√°n (VND)")

    ax.xaxis.set_major_formatter(_plain_int_formatter())
    ax.yaxis.set_major_formatter(_plain_int_formatter())
    st.pyplot(fig)


def plot_residuals(y_true, y_pred):
    residuals = y_true - y_pred
    fig, ax = plt.subplots(figsize=(6.0, 4.0))
    ax.hist(residuals, bins=30)
    ax.set_title("Ph√¢n ph·ªëi sai s·ªë (Residuals) - Validation")
    ax.set_xlabel("Sai s·ªë (gi√° th·∫≠t - gi√° d·ª± ƒëo√°n)")
    ax.set_ylabel("S·ªë l∆∞·ª£ng")

    ax.xaxis.set_major_formatter(_plain_int_formatter())
    st.pyplot(fig)


def plot_feature_importance(model_pipeline, feature_names, top_k=15):
    raw_model = model_pipeline.named_steps["model"]
    if not hasattr(raw_model, "feature_importances_"):
        st.info("Model n√†y kh√¥ng h·ªó tr·ª£ Feature Importance.")
        return

    importances = raw_model.feature_importances_
    idx = np.argsort(importances)[-top_k:]
    names = [feature_names[i] for i in idx]
    vals = importances[idx]

    fig, ax = plt.subplots(figsize=(6.0, 4.8))
    ax.barh(names, vals)
    ax.set_title(f"Top {top_k} Feature quan tr·ªçng nh·∫•t")
    ax.set_xlabel("M·ª©c ƒë·ªô quan tr·ªçng")
    st.pyplot(fig)


def predict_from_csv(trained_model, features, csv_file):
    df_in = pd.read_csv(csv_file)
    X_in = df_in.reindex(columns=features)

    # √©p numeric ƒë·ªÉ tr√°nh crash n·∫øu upload c√≥ c·ªôt object
    for c in features:
        X_in[c] = pd.to_numeric(X_in[c], errors="coerce")

    preds = trained_model.predict(X_in)
    out = df_in.copy()
    out["predicted_price_base"] = preds
    return out


# =========================
# SIDEBAR (VN)
# =========================
st.sidebar.header("1) D·ªØ li·ªáu ƒë·∫ßu v√†o")
train_up = st.sidebar.file_uploader("Upload data_train.csv", type="csv")
val_up = st.sidebar.file_uploader("Upload data_validation.csv", type="csv")
test_up = st.sidebar.file_uploader("Upload data_test.csv", type="csv")

train_path = train_up if train_up else "data_train.csv"
val_path = val_up if val_up else "data_validation.csv"
test_path = test_up if test_up else "data_test.csv"

st.sidebar.header("2) Ch·ªçn m√¥ h√¨nh & tham s·ªë")
model_choice = st.sidebar.selectbox("M√¥ h√¨nh", ["Random Forest", "XGBoost", "LightGBM"])

fast_mode = st.sidebar.checkbox("‚ö° Hu·∫•n luy·ªán nhanh (khuy·∫øn ngh·ªã)", value=True)

early_rounds = None
if model_choice == "XGBoost":
    early_rounds = st.sidebar.slider("D·ª´ng s·ªõm (Early stopping)", 10, 200, 50, 10)
    st.sidebar.caption("D·ª´ng n·∫øu Validation kh√¥ng c·∫£i thi·ªán sau N v√≤ng. Gi√∫p nhanh h∆°n v√† gi·∫£m overfit.")

params = {}

# ===== Random Forest =====
if model_choice == "Random Forest":
    params["n_estimators"] = st.sidebar.slider("S·ªë l∆∞·ª£ng c√¢y (n_estimators)", 100, 800, 500, 50)
    st.sidebar.caption("S·ªë c√¢y c√†ng nhi·ªÅu ‚Üí th∆∞·ªùng t·ªët h∆°n nh∆∞ng train l√¢u h∆°n. G·ª£i √Ω: 400‚Äì800.")

    params["max_depth"] = st.sidebar.slider("ƒê·ªô s√¢u t·ªëi ƒëa (max_depth) - 0 = kh√¥ng gi·ªõi h·∫°n", 0, 40, 0, 1)
    st.sidebar.caption("Depth l·ªõn ‚Üí m√¥ h√¨nh ph·ª©c t·∫°p h∆°n, d·ªÖ overfit. G·ª£i √Ω: 10‚Äì20 ho·∫∑c 0 n·∫øu mu·ªën th·ª≠.")

    params["min_samples_split"] = st.sidebar.slider("S·ªë m·∫´u t·ªëi thi·ªÉu ƒë·ªÉ t√°ch nh√°nh (min_samples_split)", 2, 20, 2, 1)
    st.sidebar.caption("TƒÉng l√™n ‚Üí gi·∫£m overfit (c√¢y √≠t t√°ch nh√°nh h∆°n).")

    params["min_samples_leaf"] = st.sidebar.slider("S·ªë m·∫´u t·ªëi thi·ªÉu t·∫°i l√° (min_samples_leaf)", 1, 20, 1, 1)
    st.sidebar.caption("TƒÉng l√™n ‚Üí ·ªïn ƒë·ªãnh h∆°n nh∆∞ng c√≥ th·ªÉ gi·∫£m ƒë·ªô kh·ªõp.")

    params["max_features"] = st.sidebar.slider("T·ªâ l·ªá feature m·ªói c√¢y (max_features)", 0.2, 1.0, 0.7, 0.05)
    st.sidebar.caption("Gi·∫£m xu·ªëng gi√∫p ch·ªëng overfit. G·ª£i √Ω: 0.5‚Äì0.8.")

# ===== XGBoost =====
elif model_choice == "XGBoost":
    # default ‚Äúm·∫°nh‚Äù h∆°n ƒë·ªÉ b·∫°n d·ªÖ l√™n R2
    params["n_estimators"] = st.sidebar.slider("S·ªë v√≤ng boosting (n_estimators)", 300, 4000, 2000, 100)
    st.sidebar.caption("Nhi·ªÅu v√≤ng ‚Üí m√¥ h√¨nh m·∫°nh h∆°n nh∆∞ng ch·∫≠m h∆°n. N√™n d√πng d·ª´ng s·ªõm ƒë·ªÉ t·ª± ng·∫Øt.")

    params["max_depth"] = st.sidebar.slider("ƒê·ªô s√¢u c√¢y (max_depth)", 2, 12, 6, 1)
    st.sidebar.caption("Depth l·ªõn ‚Üí m·∫°nh h∆°n nh∆∞ng d·ªÖ overfit. G·ª£i √Ω: 4‚Äì8.")

    params["learning_rate"] = st.sidebar.number_input("T·ªëc ƒë·ªô h·ªçc (learning_rate)", 0.005, 0.3, 0.03, step=0.005)
    st.sidebar.caption("Learning rate nh·ªè ‚Üí ·ªïn ƒë·ªãnh h∆°n nh∆∞ng c·∫ßn nhi·ªÅu v√≤ng h∆°n. G·ª£i √Ω: 0.02‚Äì0.08.")

    params["subsample"] = st.sidebar.slider("T·ªâ l·ªá l·∫•y m·∫´u d·ªØ li·ªáu (subsample)", 0.5, 1.0, 0.9, 0.05)
    st.sidebar.caption("Gi·∫£m <1.0 gi√∫p ch·ªëng overfit.")

    params["colsample_bytree"] = st.sidebar.slider("T·ªâ l·ªá l·∫•y m·∫´u feature (colsample_bytree)", 0.5, 1.0, 0.9, 0.05)
    st.sidebar.caption("Gi·∫£m <1.0 gi√∫p ch·ªëng overfit.")

    params["min_child_weight"] = st.sidebar.number_input("Min child weight", 0.0, 50.0, 1.0, step=0.5)
    st.sidebar.caption("TƒÉng l√™n n·∫øu overfit (y√™u c·∫ßu node ph·∫£i ƒë·ªß ‚Äòn·∫∑ng‚Äô m·ªõi t√°ch).")

    params["gamma"] = st.sidebar.number_input("Gamma", 0.0, 20.0, 0.0, step=0.1)
    st.sidebar.caption("TƒÉng gamma ‚Üí kh√≥ t√°ch nh√°nh h∆°n ‚Üí gi·∫£m overfit.")

    params["reg_alpha"] = st.sidebar.number_input("Ph·∫°t L1 (reg_alpha)", 0.0, 10.0, 0.0, step=0.1)
    st.sidebar.caption("TƒÉng n·∫øu feature nhi·ªÖu/overfit.")

    params["reg_lambda"] = st.sidebar.number_input("Ph·∫°t L2 (reg_lambda)", 0.0, 10.0, 2.0, step=0.1)
    st.sidebar.caption("TƒÉng ƒë·ªÉ m√¥ h√¨nh ‚Äòm∆∞·ª£t‚Äô h∆°n v√† gi·∫£m overfit.")

# ===== LightGBM =====
else:
    params["n_estimators"] = st.sidebar.slider("S·ªë v√≤ng boosting (n_estimators)", 300, 8000, 3000, 100)
    st.sidebar.caption("Nhi·ªÅu v√≤ng ‚Üí c√≥ th·ªÉ t·ªët h∆°n nh∆∞ng ch·∫≠m h∆°n. G·ª£i √Ω: 1500‚Äì4000.")

    params["max_depth"] = st.sidebar.slider("ƒê·ªô s√¢u t·ªëi ƒëa (max_depth) - 0 = kh√¥ng gi·ªõi h·∫°n", 0, 30, 0, 1)
    st.sidebar.caption("Gi·ªõi h·∫°n depth ƒë·ªÉ tr√°nh overfit. G·ª£i √Ω: 6‚Äì12 ho·∫∑c 0 n·∫øu mu·ªën th·ª≠.")

    params["learning_rate"] = st.sidebar.number_input("T·ªëc ƒë·ªô h·ªçc (learning_rate)", 0.005, 0.3, 0.03, step=0.005)
    st.sidebar.caption("Nh·ªè h∆°n ‚Üí ·ªïn ƒë·ªãnh h∆°n nh∆∞ng c·∫ßn nhi·ªÅu v√≤ng h∆°n. G·ª£i √Ω: 0.02‚Äì0.08.")

    params["num_leaves"] = st.sidebar.slider("S·ªë l√° t·ªëi ƒëa (num_leaves)", 15, 255, 63, 2)
    st.sidebar.caption("num_leaves l·ªõn ‚Üí m√¥ h√¨nh m·∫°nh h∆°n nh∆∞ng d·ªÖ overfit. G·ª£i √Ω: 31‚Äì127.")

    params["subsample"] = st.sidebar.slider("T·ªâ l·ªá l·∫•y m·∫´u d·ªØ li·ªáu (subsample)", 0.5, 1.0, 0.9, 0.05)
    st.sidebar.caption("Gi·∫£m <1.0 gi√∫p ch·ªëng overfit.")

    params["colsample_bytree"] = st.sidebar.slider("T·ªâ l·ªá l·∫•y m·∫´u feature (colsample_bytree)", 0.5, 1.0, 0.9, 0.05)
    st.sidebar.caption("Gi·∫£m <1.0 gi√∫p ch·ªëng overfit.")

    params["min_child_samples"] = st.sidebar.slider("Min child samples", 5, 200, 20, 5)
    st.sidebar.caption("TƒÉng l√™n n·∫øu overfit (l√° ph·∫£i c√≥ ƒë·ªß m·∫´u m·ªõi t√°ch).")

    params["reg_alpha"] = st.sidebar.number_input("Ph·∫°t L1 (reg_alpha)", 0.0, 10.0, 0.0, step=0.1)
    st.sidebar.caption("TƒÉng n·∫øu d·ªØ li·ªáu nhi·ªÖu/overfit.")

    params["reg_lambda"] = st.sidebar.number_input("Ph·∫°t L2 (reg_lambda)", 0.0, 10.0, 0.0, step=0.1)
    st.sidebar.caption("TƒÉng n·∫øu mu·ªën m√¥ h√¨nh ·ªïn ƒë·ªãnh h∆°n.")


# =========================
# MAIN
# =========================
st.title("üíª ·ª®ng d·ª•ng D·ª± ƒëo√°n Gi√° Laptop")

def _available(p):
    return (hasattr(p, "read")) or os.path.exists(str(p))

if not all(_available(p) for p in [train_path, val_path, test_path]):
    st.info("Vui l√≤ng ƒë·∫∑t ƒë·ªß 3 file: data_train.csv, data_validation.csv, data_test.csv (ho·∫∑c upload ·ªü sidebar).")
    st.stop()

try:
    df_tr, df_vl, df_ts = load_data(train_path, val_path, test_path)
    X_tr, y_tr, X_vl, y_vl, X_ts, y_ts, features = align_columns(df_tr, df_vl, df_ts)
except Exception as e:
    st.error(f"L·ªói x·ª≠ l√Ω d·ªØ li·ªáu: {e}")
    st.stop()

tab1, tab2, tab3, tab4 = st.tabs(["üìå Xem d·ªØ li·ªáu", "üß† Hu·∫•n luy·ªán & ƒê√°nh gi√°", "üéØ D·ª± ƒëo√°n", "üì§ Xu·∫•t file"])

with tab1:
    st.subheader("Xem nhanh d·ªØ li·ªáu (head)")
    c1, c2, c3 = st.columns(3)
    with c1:
        st.markdown("### Train")
        st.write("K√≠ch th∆∞·ªõc:", df_tr.shape)
        st.dataframe(df_tr.head(10), use_container_width=True)
    with c2:
        st.markdown("### Validation")
        st.write("K√≠ch th∆∞·ªõc:", df_vl.shape)
        st.dataframe(df_vl.head(10), use_container_width=True)
    with c3:
        st.markdown("### Test")
        st.write("K√≠ch th∆∞·ªõc:", df_ts.shape)
        st.dataframe(df_ts.head(10), use_container_width=True)

    st.info(f"ƒê√£ l·∫•y **c·ªôt chung (intersection)** gi·ªØa train/val/test: **{len(features)} features** (ƒë√£ lo·∫°i 'title' v√† target).")


with tab2:
    st.subheader("Hu·∫•n luy·ªán m√¥ h√¨nh")
    start_train = st.button("üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán", type="primary")

    if start_train:
        with st.spinner("ƒêang hu·∫•n luy·ªán..."):
            model_pipeline, use_es = build_model(model_choice, params)
            model_pipeline, y_pred_vl = train_and_eval(
                model_pipeline, use_es, model_choice, X_tr, y_tr, X_vl, y_vl,
                early_rounds=(early_rounds if early_rounds is not None else 50)
            )

            st.session_state["trained_model"] = model_pipeline
            st.session_state["features"] = features
            st.session_state["y_pred_vl"] = y_pred_vl

            metrics_vl = calculate_metrics(y_vl.values, y_pred_vl)
            st.session_state["metrics_vl"] = metrics_vl

        st.success("‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t!")

    if "trained_model" in st.session_state:
        st.markdown("### K·∫øt qu·∫£ tr√™n Validation")
        metrics_vl = st.session_state["metrics_vl"]

        m_cols = st.columns(4)
        keys = list(metrics_vl.keys())
        for i, k in enumerate(keys):
            m_cols[i].metric(k, f"{metrics_vl[k]:,.4f}" if k == "R2" else f"{metrics_vl[k]:,.0f}" if k in ["MAE","RMSE"] else f"{metrics_vl[k]:,.2f}")

        # ch√∫ th√≠ch metric
        st.caption("**R2**: c√†ng g·∫ßn 1 c√†ng t·ªët (m√¥ h√¨nh gi·∫£i th√≠ch ƒë∆∞·ª£c bi·∫øn ƒë·ªông gi√°).")
        st.caption("**MAE**: sai s·ªë tuy·ªát ƒë·ªëi trung b√¨nh (VND) ‚Äî c√†ng nh·ªè c√†ng t·ªët.")
        st.caption("**RMSE**: gi·ªëng MAE nh∆∞ng ph·∫°t n·∫∑ng l·ªói l·ªõn (VND) ‚Äî c√†ng nh·ªè c√†ng t·ªët.")
        st.caption("**MAPE**: % sai s·ªë trung b√¨nh so v·ªõi gi√° th·∫≠t ‚Äî c√†ng nh·ªè c√†ng t·ªët.")

        st.markdown("### Bi·ªÉu ƒë·ªì (1 ·∫£nh / 1 h√†ng)")
        plot_scatter(y_vl.values, st.session_state["y_pred_vl"])
        plot_residuals(y_vl.values, st.session_state["y_pred_vl"])
        st.markdown("#### Feature Importance")
        plot_feature_importance(st.session_state["trained_model"], features, top_k=15)

        st.divider()
        if st.button("üîç ƒê√°nh gi√° th√™m tr√™n Test"):
            y_pred_ts = st.session_state["trained_model"].predict(X_ts)
            metrics_ts = calculate_metrics(y_ts.values, y_pred_ts)
            st.markdown("### K·∫øt qu·∫£ tr√™n Test")
            st.json(metrics_ts)
            st.session_state["test_pred"] = y_pred_ts
    else:
        st.info("B·∫•m **B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán** ƒë·ªÉ train model.")


with tab3:
    st.subheader("D·ª± ƒëo√°n b·∫±ng Upload CSV (kh√¥ng nh·∫≠p tay)")
    if "trained_model" not in st.session_state:
        st.info("B·∫°n c·∫ßn hu·∫•n luy·ªán model tr∆∞·ªõc.")
    else:
        st.write("‚úÖ Upload 1 file CSV ƒë·ªÉ d·ª± ƒëo√°n (1 d√≤ng ho·∫∑c nhi·ªÅu d√≤ng).")
        st.write("App s·∫Ω t·ª± **align c·ªôt** theo features l√∫c train (th·ª´a c·ªôt b·ªè, thi·∫øu c·ªôt ‚Üí NaN v√† imputer x·ª≠ l√Ω).")

        pred_file = st.file_uploader("Upload CSV ƒë·ªÉ d·ª± ƒëo√°n", type="csv", key="pred_csv")
        if pred_file:
            out_df = predict_from_csv(st.session_state["trained_model"], st.session_state["features"], pred_file)
            st.success("‚úÖ D·ª± ƒëo√°n xong!")
            st.dataframe(out_df.head(20), use_container_width=True)

            csv_bytes = out_df.to_csv(index=False).encode("utf-8")
            st.download_button("‚¨áÔ∏è T·∫£i file d·ª± ƒëo√°n", csv_bytes, "predictions.csv", "text/csv")


with tab4:
    st.subheader("Xu·∫•t model v√† d·ª± ƒëo√°n test")
    if "trained_model" not in st.session_state:
        st.info("B·∫°n c·∫ßn hu·∫•n luy·ªán model tr∆∞·ªõc.")
    else:
        joblib.dump(st.session_state["trained_model"], "model.joblib")
        with open("model.joblib", "rb") as f:
            st.download_button("üíæ T·∫£i model.joblib", f, "model.joblib")

        st.divider()

        if "test_pred" not in st.session_state:
            st.info("H√£y b·∫•m **ƒê√°nh gi√° th√™m tr√™n Test** ·ªü tab Hu·∫•n luy·ªán ƒë·ªÉ t·∫°o file d·ª± ƒëo√°n test.")
        else:
            test_results = df_ts.copy()
            test_results["predicted_price_base"] = st.session_state["test_pred"]
            csv = test_results.to_csv(index=False).encode("utf-8")
            st.download_button("üìä T·∫£i test_predictions.csv", csv, "test_predictions.csv", "text/csv")
