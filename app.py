import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import joblib
import os

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import HistGradientBoostingRegressor  # theo notebook báº¡n
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from matplotlib.ticker import FuncFormatter

# =========================
# CONFIG
# =========================
st.set_page_config(page_title="á»¨ng dá»¥ng Dá»± Ä‘oÃ¡n GiÃ¡ Laptop", layout="wide")


# =========================
# FUNCTIONS
# =========================
def load_data(train_file, val_file, test_file):
    df_train = pd.read_csv(train_file)
    df_val = pd.read_csv(val_file)
    df_test = pd.read_csv(test_file)
    return df_train, df_val, df_test


def align_columns(df_train, df_val, df_test, target="price_base"):
    # Drop title náº¿u cÃ³
    def _drop_title(df):
        if "title" in df.columns:
            return df.drop(columns=["title"])
        return df

    df_train = _drop_title(df_train)
    df_val = _drop_title(df_val)
    df_test = _drop_title(df_test)

    # intersection columns (trÃ¡nh test thá»«a cá»™t)
    common_cols = list(set(df_train.columns) & set(df_val.columns) & set(df_test.columns))

    if target not in df_train.columns or target not in df_val.columns or target not in df_test.columns:
        raise ValueError(f"Thiáº¿u cá»™t target '{target}' trong 1 trong 3 táº­p dá»¯ liá»‡u.")

    if target in common_cols:
        common_cols.remove(target)

    common_cols.sort()

    X_train = df_train[common_cols].copy()
    y_train = df_train[target].copy()

    X_val = df_val[common_cols].copy()
    y_val = df_val[target].copy()

    X_test = df_test[common_cols].copy()
    y_test = df_test[target].copy()

    return X_train, y_train, X_val, y_val, X_test, y_test, common_cols


def calculate_metrics(y_true, y_pred):
    r2 = float(r2_score(y_true, y_pred))
    mae = float(mean_absolute_error(y_true, y_pred))
    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))

    # Safe MAPE (trÃ¡nh chia 0 hoáº·c quÃ¡ nhá»)
    eps = 1e-8
    denom = np.maximum(np.abs(y_true), eps)
    mape = float(np.mean(np.abs((y_true - y_pred) / denom)) * 100)

    return {"R2": r2, "MAE": mae, "RMSE": rmse, "MAPE (%)": mape}


def build_model(model_type, params):
    """
    Giá»¯ nguyÃªn flow: tráº£ vá» pipeline + cá» early_stop
    """

    if model_type == "Random Forest":
        base_model = RandomForestRegressor(
            n_estimators=int(params["n_estimators"]),
            max_depth=None if int(params["max_depth"]) == 0 else int(params["max_depth"]),
            min_samples_split=int(params["min_samples_split"]),
            min_samples_leaf=int(params["min_samples_leaf"]),
            n_jobs=-1,
            random_state=42
        )
        use_early_stop = False

    elif model_type == "XGBoost":
        base_model = XGBRegressor(
            n_estimators=int(params["n_estimators"]),
            max_depth=int(params["max_depth"]),
            learning_rate=float(params["learning_rate"]),
            subsample=float(params["subsample"]),
            colsample_bytree=float(params["colsample_bytree"]),
            reg_alpha=float(params["reg_alpha"]),
            reg_lambda=float(params["reg_lambda"]),
            objective="reg:squarederror",
            eval_metric="rmse",
            n_jobs=-1,
            random_state=42
        )
        use_early_stop = True

    elif model_type == "LightGBM":
        base_model = LGBMRegressor(
            n_estimators=int(params["n_estimators"]),
            max_depth=-1 if int(params["max_depth"]) == 0 else int(params["max_depth"]),
            learning_rate=float(params["learning_rate"]),
            num_leaves=int(params["num_leaves"]),
            subsample=float(params["subsample"]),
            colsample_bytree=float(params["colsample_bytree"]),
            reg_alpha=float(params["reg_alpha"]),
            reg_lambda=float(params["reg_lambda"]),
            n_jobs=-1,
            random_state=42
        )
        use_early_stop = True

    else:  # HistGradientBoosting (theo notebook báº¡n)
        base_model = HistGradientBoostingRegressor(
            learning_rate=float(params["learning_rate"]),
            max_iter=int(params["max_iter"]),
            max_depth=None if int(params["max_depth"]) == 0 else int(params["max_depth"]),
            random_state=42
        )
        use_early_stop = False

    pipeline = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("model", base_model)
    ])
    return pipeline, use_early_stop


def train_and_eval(model_pipeline, use_early_stop, model_type, X_tr, y_tr, X_vl, y_vl, early_rounds=50):
    """
    - KhÃ´ng Ä‘á»ƒ app crash
    - XGBoost: thá»­ early_stopping_rounds / callback tÃ¹y version, náº¿u fail -> train khÃ´ng early stop
    """
    if use_early_stop:
        X_tr_imp = model_pipeline.named_steps["imputer"].fit_transform(X_tr)
        X_vl_imp = model_pipeline.named_steps["imputer"].transform(X_vl)
        model = model_pipeline.named_steps["model"]

        if model_type == "XGBoost":
            # 1) thá»­ early_stopping_rounds (nhiá»u báº£n xgboost support)
            try:
                model.fit(
                    X_tr_imp, y_tr,
                    eval_set=[(X_vl_imp, y_vl)],
                    verbose=False,
                    early_stopping_rounds=int(early_rounds)
                )
            except TypeError:
                # 2) fallback: train bÃ¬nh thÆ°á»ng (KHÃ”NG early stopping) Ä‘á»ƒ khÃ´ng crash
                model.fit(X_tr_imp, y_tr)

            y_pred_vl = model.predict(X_vl_imp)
            return model_pipeline, y_pred_vl

        else:  # LightGBM
            # LightGBM sklearn API á»•n Ä‘á»‹nh, train nhanh
            model.fit(
                X_tr_imp, y_tr,
                eval_set=[(X_vl_imp, y_vl)],
                eval_metric="l2",
            )
            y_pred_vl = model.predict(X_vl_imp)
            return model_pipeline, y_pred_vl

    # RF / HistGB
    model_pipeline.fit(X_tr, y_tr)
    y_pred_vl = model_pipeline.predict(X_vl)
    return model_pipeline, y_pred_vl


def _plain_number_formatter():
    # hiá»ƒn thá»‹ 15690000 thay vÃ¬ 1.569e7
    return FuncFormatter(lambda x, pos: f"{int(x):d}")


def plot_scatter(y_true, y_pred):
    fig, ax = plt.subplots(figsize=(6.2, 4.2))
    ax.scatter(y_true, y_pred, alpha=0.5)

    mn = float(min(np.min(y_true), np.min(y_pred)))
    mx = float(max(np.max(y_true), np.max(y_pred)))
    ax.plot([mn, mx], [mn, mx], "r--")

    ax.set_title("So sÃ¡nh GiÃ¡ Tháº­t vs GiÃ¡ Dá»± Ä‘oÃ¡n (Validation)")
    ax.set_xlabel("GiÃ¡ tháº­t")
    ax.set_ylabel("GiÃ¡ dá»± Ä‘oÃ¡n")

    ax.xaxis.set_major_formatter(_plain_number_formatter())
    ax.yaxis.set_major_formatter(_plain_number_formatter())
    ax.ticklabel_format(style="plain", axis="both", useOffset=False)

    st.pyplot(fig)


def plot_residuals(y_true, y_pred):
    residuals = y_true - y_pred
    fig, ax = plt.subplots(figsize=(6.2, 4.2))
    ax.hist(residuals, bins=30)
    ax.set_title("PhÃ¢n phá»‘i Sai sá»‘ (Residuals) - Validation")
    ax.set_xlabel("Sai sá»‘ (giÃ¡ tháº­t - giÃ¡ dá»± Ä‘oÃ¡n)")
    ax.set_ylabel("Sá»‘ lÆ°á»£ng")

    ax.xaxis.set_major_formatter(_plain_number_formatter())
    ax.ticklabel_format(style="plain", axis="x", useOffset=False)

    st.pyplot(fig)


def plot_feature_importance(model_pipeline, feature_names, top_k=15):
    raw_model = model_pipeline.named_steps["model"]
    if not hasattr(raw_model, "feature_importances_"):
        st.info("Model nÃ y khÃ´ng há»— trá»£ Feature Importance.")
        return

    importances = raw_model.feature_importances_
    idx = np.argsort(importances)[-top_k:]
    names = [feature_names[i] for i in idx]
    vals = importances[idx]

    fig, ax = plt.subplots(figsize=(6.2, 4.8))
    ax.barh(names, vals)
    ax.set_title(f"Top {top_k} Feature Importance")
    ax.set_xlabel("Má»©c Ä‘á»™ quan trá»ng")
    st.pyplot(fig)


def predict_from_csv(trained_model, features, csv_file):
    df_in = pd.read_csv(csv_file)
    X_in = df_in.reindex(columns=features)
    preds = trained_model.predict(X_in)
    out = df_in.copy()
    out["predicted_price_base"] = preds
    return out


# =========================
# SIDEBAR (VN)
# =========================
st.sidebar.header("1) Dá»¯ liá»‡u Ä‘áº§u vÃ o")
train_up = st.sidebar.file_uploader("Upload data_train.csv", type="csv")
val_up = st.sidebar.file_uploader("Upload data_validation.csv", type="csv")
test_up = st.sidebar.file_uploader("Upload data_test.csv", type="csv")

train_path = train_up if train_up else "data_train.csv"
val_path = val_up if val_up else "data_validation.csv"
test_path = test_up if test_up else "data_test.csv"

st.sidebar.header("2) Chá»n mÃ´ hÃ¬nh & tham sá»‘")

# âœ… thÃªm HistGB theo notebook báº¡n (Ä‘á»ƒ ra ~0.85 giá»‘ng báº¡n nÃ³i)
model_choice = st.sidebar.selectbox(
    "MÃ´ hÃ¬nh",
    ["Random Forest", "XGBoost", "LightGBM", "HistGradientBoosting (theo notebook cá»§a báº¡n)"]
)

fast_mode = st.sidebar.checkbox("âš¡ Huáº¥n luyá»‡n nhanh (khuyáº¿n nghá»‹)", value=True)

early_rounds = None
if model_choice == "XGBoost":
    early_rounds = st.sidebar.slider("Dá»«ng sá»›m (Early stopping)", 10, 200, 50, 10)
    st.sidebar.caption(
        "Tá»± Ä‘á»™ng dá»«ng náº¿u mÃ´ hÃ¬nh khÃ´ng cáº£i thiá»‡n trÃªn Validation sau N vÃ²ng (náº¿u mÃ´i trÆ°á»ng há»— trá»£)."
    )

params = {}

# ===== Random Forest =====
if model_choice == "Random Forest":
    params["n_estimators"] = st.sidebar.slider(
        "Sá»‘ lÆ°á»£ng cÃ¢y (n_estimators)", 50, 600, 400 if fast_mode else 500, 50
    )
    st.sidebar.caption("Sá»‘ cÃ¢y nhiá»u hÆ¡n â†’ thÆ°á»ng tá»‘t hÆ¡n nhÆ°ng train cháº­m hÆ¡n. Gá»£i Ã½: 300â€“600.")

    params["max_depth"] = st.sidebar.slider(
        "Äá»™ sÃ¢u tá»‘i Ä‘a (max_depth) - 0 = khÃ´ng giá»›i háº¡n", 0, 30, 0 if fast_mode else 12, 1
    )
    st.sidebar.caption("Giá»›i háº¡n Ä‘á»™ sÃ¢u Ä‘á»ƒ giáº£m overfit. Gá»£i Ã½: 8â€“16 hoáº·c 0 náº¿u muá»‘n thá»­.")

    params["min_samples_split"] = st.sidebar.slider(
        "Sá»‘ máº«u tá»‘i thiá»ƒu Ä‘á»ƒ tÃ¡ch nhÃ¡nh (min_samples_split)", 2, 20, 2, 1
    )
    st.sidebar.caption("TÄƒng giÃ¡ trá»‹ nÃ y â†’ giáº£m overfit.")

    params["min_samples_leaf"] = st.sidebar.slider(
        "Sá»‘ máº«u tá»‘i thiá»ƒu táº¡i lÃ¡ (min_samples_leaf)", 1, 20, 1, 1
    )
    st.sidebar.caption("TÄƒng giÃ¡ trá»‹ nÃ y â†’ mÃ´ hÃ¬nh á»•n Ä‘á»‹nh hÆ¡n nhÆ°ng cÃ³ thá»ƒ giáº£m Ä‘á»™ khá»›p.")

# ===== XGBoost =====
elif model_choice == "XGBoost":
    # âœ… máº·c Ä‘á»‹nh â€œgáº§n tá»‘i Æ°uâ€ Ä‘á»ƒ báº¡n dá»… lÃªn R2 (báº¡n chá»‰nh Ä‘Æ°á»£c)
    params["n_estimators"] = st.sidebar.slider(
        "Sá»‘ vÃ²ng boosting (n_estimators)", 200, 2500, 1200 if fast_mode else 1800, 100
    )
    st.sidebar.caption("Sá»‘ vÃ²ng cÃ ng nhiá»u â†’ mÃ´ hÃ¬nh cÃ ng máº¡nh nhÆ°ng dá»… overfit. DÃ¹ng dá»«ng sá»›m Ä‘á»ƒ tá»± ngáº¯t.")

    params["max_depth"] = st.sidebar.slider("Äá»™ sÃ¢u cÃ¢y (max_depth)", 2, 12, 6, 1)
    st.sidebar.caption("Äá»™ sÃ¢u lá»›n â†’ máº¡nh hÆ¡n nhÆ°ng dá»… overfit. Gá»£i Ã½: 4â€“8.")

    params["learning_rate"] = st.sidebar.number_input(
        "Tá»‘c Ä‘á»™ há»c (learning_rate)", 0.005, 0.3, 0.05, step=0.005
    )
    st.sidebar.caption("Nhá» hÆ¡n â†’ á»•n Ä‘á»‹nh hÆ¡n nhÆ°ng cáº§n nhiá»u vÃ²ng hÆ¡n. Gá»£i Ã½: 0.03â€“0.08.")

    params["subsample"] = st.sidebar.slider("Tá»‰ lá»‡ láº¥y máº«u dá»¯ liá»‡u (subsample)", 0.5, 1.0, 0.9, 0.05)
    st.sidebar.caption("Giáº£m <1.0 giÃºp chá»‘ng overfit.")

    params["colsample_bytree"] = st.sidebar.slider("Tá»‰ lá»‡ láº¥y máº«u feature (colsample_bytree)", 0.5, 1.0, 0.9, 0.05)
    st.sidebar.caption("Giáº£m <1.0 giÃºp chá»‘ng overfit.")

    params["reg_alpha"] = st.sidebar.number_input("Pháº¡t L1 (reg_alpha)", 0.0, 10.0, 0.0, step=0.1)
    st.sidebar.caption("TÄƒng náº¿u dá»¯ liá»‡u nhiá»…u hoáº·c overfit.")

    params["reg_lambda"] = st.sidebar.number_input("Pháº¡t L2 (reg_lambda)", 0.0, 10.0, 2.0, step=0.1)
    st.sidebar.caption("TÄƒng Ä‘á»ƒ mÃ´ hÃ¬nh â€˜mÆ°á»£tâ€™ hÆ¡n vÃ  giáº£m overfit.")

# ===== LightGBM =====
elif model_choice == "LightGBM":
    params["n_estimators"] = st.sidebar.slider(
        "Sá»‘ vÃ²ng boosting (n_estimators)", 200, 5000, 1200 if fast_mode else 2500, 100
    )
    st.sidebar.caption("Nhiá»u vÃ²ng hÆ¡n â†’ cÃ³ thá»ƒ tá»‘t hÆ¡n nhÆ°ng cháº­m hÆ¡n. Gá»£i Ã½: 800â€“2500.")

    params["max_depth"] = st.sidebar.slider(
        "Äá»™ sÃ¢u tá»‘i Ä‘a (max_depth) - 0 = khÃ´ng giá»›i háº¡n", 0, 30, 10 if fast_mode else 12, 1
    )
    st.sidebar.caption("Giá»›i háº¡n depth Ä‘á»ƒ trÃ¡nh overfit. Gá»£i Ã½: 6â€“12.")

    params["learning_rate"] = st.sidebar.number_input(
        "Tá»‘c Ä‘á»™ há»c (learning_rate)", 0.005, 0.3, 0.05, step=0.005
    )
    st.sidebar.caption("Nhá» hÆ¡n â†’ á»•n Ä‘á»‹nh hÆ¡n nhÆ°ng cáº§n nhiá»u vÃ²ng hÆ¡n. Gá»£i Ã½: 0.03â€“0.08.")

    params["num_leaves"] = st.sidebar.slider("Sá»‘ lÃ¡ tá»‘i Ä‘a (num_leaves)", 15, 127, 63 if fast_mode else 63, 2)
    st.sidebar.caption("num_leaves lá»›n â†’ máº¡nh hÆ¡n nhÆ°ng dá»… overfit. Gá»£i Ã½: 31â€“63.")

    params["subsample"] = st.sidebar.slider("Tá»‰ lá»‡ láº¥y máº«u dá»¯ liá»‡u (subsample)", 0.5, 1.0, 0.9, 0.05)
    st.sidebar.caption("Giáº£m <1.0 giÃºp chá»‘ng overfit.")

    params["colsample_bytree"] = st.sidebar.slider("Tá»‰ lá»‡ láº¥y máº«u feature (colsample_bytree)", 0.5, 1.0, 0.9, 0.05)
    st.sidebar.caption("Giáº£m <1.0 giÃºp chá»‘ng overfit.")

    params["reg_alpha"] = st.sidebar.number_input("Pháº¡t L1 (reg_alpha)", 0.0, 10.0, 0.0, step=0.1)
    st.sidebar.caption("TÄƒng náº¿u dá»¯ liá»‡u nhiá»…u hoáº·c overfit.")

    params["reg_lambda"] = st.sidebar.number_input("Pháº¡t L2 (reg_lambda)", 0.0, 10.0, 0.0, step=0.1)
    st.sidebar.caption("TÄƒng náº¿u muá»‘n mÃ´ hÃ¬nh á»•n Ä‘á»‹nh hÆ¡n.")

# ===== HistGradientBoosting (theo notebook báº¡n) =====
else:
    # âœ… máº·c Ä‘á»‹nh Ä‘Ãºng notebook cá»§a báº¡n: learning_rate=0.1, max_iter=100, max_depth=5
    params["learning_rate"] = st.sidebar.number_input(
        "Tá»‘c Ä‘á»™ há»c (learning_rate)", 0.01, 0.3, 0.10, step=0.01
    )
    st.sidebar.caption("Notebook báº¡n dÃ¹ng 0.1. Nhá» hÆ¡n â†’ á»•n Ä‘á»‹nh hÆ¡n nhÆ°ng cáº§n nhiá»u vÃ²ng hÆ¡n.")

    params["max_iter"] = st.sidebar.slider("Sá»‘ vÃ²ng láº·p (max_iter)", 50, 400, 100, 25)
    st.sidebar.caption("Notebook báº¡n dÃ¹ng 100. TÄƒng lÃªn cÃ³ thá»ƒ tá»‘t hÆ¡n nhÆ°ng cháº­m hÆ¡n.")

    params["max_depth"] = st.sidebar.slider("Äá»™ sÃ¢u tá»‘i Ä‘a (max_depth) - 0 = khÃ´ng giá»›i háº¡n", 0, 20, 5, 1)
    st.sidebar.caption("Notebook báº¡n dÃ¹ng 5. Giá»›i háº¡n depth giÃºp giáº£m overfit.")


# =========================
# MAIN
# =========================
st.title("ğŸ’» á»¨ng dá»¥ng Dá»± Ä‘oÃ¡n GiÃ¡ Laptop")

def _available(p):
    return (hasattr(p, "read")) or os.path.exists(str(p))

if not all(_available(p) for p in [train_path, val_path, test_path]):
    st.info("Vui lÃ²ng Ä‘áº·t Ä‘á»§ 3 file: data_train.csv, data_validation.csv, data_test.csv (hoáº·c upload á»Ÿ sidebar).")
    st.stop()

try:
    df_tr, df_vl, df_ts = load_data(train_path, val_path, test_path)
    X_tr, y_tr, X_vl, y_vl, X_ts, y_ts, features = align_columns(df_tr, df_vl, df_ts)
except Exception as e:
    st.error(f"Lá»—i xá»­ lÃ½ dá»¯ liá»‡u: {e}")
    st.stop()

tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Œ Xem dá»¯ liá»‡u", "ğŸ§  Huáº¥n luyá»‡n & ÄÃ¡nh giÃ¡", "ğŸ¯ Dá»± Ä‘oÃ¡n", "ğŸ“¤ Xuáº¥t file"])

with tab1:
    st.subheader("Xem nhanh dá»¯ liá»‡u (head)")
    c1, c2, c3 = st.columns(3)
    with c1:
        st.markdown("### Train")
        st.write("KÃ­ch thÆ°á»›c:", df_tr.shape)
        st.dataframe(df_tr.head(10), use_container_width=True)
    with c2:
        st.markdown("### Validation")
        st.write("KÃ­ch thÆ°á»›c:", df_vl.shape)
        st.dataframe(df_vl.head(10), use_container_width=True)
    with c3:
        st.markdown("### Test")
        st.write("KÃ­ch thÆ°á»›c:", df_ts.shape)
        st.dataframe(df_ts.head(10), use_container_width=True)

    st.info(f"ÄÃ£ láº¥y **cá»™t chung (intersection)** giá»¯a train/val/test: **{len(features)} features** (Ä‘Ã£ loáº¡i 'title' vÃ  target).")


with tab2:
    st.subheader("Huáº¥n luyá»‡n mÃ´ hÃ¬nh")
    start_train = st.button("ğŸš€ Báº¯t Ä‘áº§u huáº¥n luyá»‡n", type="primary")

    if start_train:
        with st.spinner("Äang huáº¥n luyá»‡n..."):
            model_pipeline, use_es = build_model(model_choice, params)
            model_pipeline, y_pred_vl = train_and_eval(
                model_pipeline, use_es, model_choice, X_tr, y_tr, X_vl, y_vl,
                early_rounds=(early_rounds if early_rounds is not None else 50)
            )

            st.session_state["trained_model"] = model_pipeline
            st.session_state["features"] = features
            st.session_state["y_pred_vl"] = y_pred_vl

            metrics_vl = calculate_metrics(y_vl.values, y_pred_vl)
            st.session_state["metrics_vl"] = metrics_vl

        st.success("âœ… Huáº¥n luyá»‡n hoÃ n táº¥t!")

    if "trained_model" in st.session_state:
        st.markdown("### Káº¿t quáº£ trÃªn Validation")
        metrics_vl = st.session_state["metrics_vl"]

        m_cols = st.columns(4)
        keys = list(metrics_vl.keys())
        for i, k in enumerate(keys):
            m_cols[i].metric(k, f"{metrics_vl[k]:,.4f}" if k == "R2" else f"{metrics_vl[k]:,.2f}")

        # âœ… chÃº thÃ­ch metric theo yÃªu cáº§u
        st.caption("**R2**: cÃ ng gáº§n 1 cÃ ng tá»‘t (mÃ´ hÃ¬nh giáº£i thÃ­ch Ä‘Æ°á»£c biáº¿n Ä‘á»™ng giÃ¡).")
        st.caption("**MAE**: sai sá»‘ tuyá»‡t Ä‘á»‘i trung bÃ¬nh (Ä‘Æ¡n vá»‹: VND) â€” cÃ ng nhá» cÃ ng tá»‘t.")
        st.caption("**RMSE**: giá»‘ng MAE nhÆ°ng pháº¡t náº·ng lá»—i lá»›n hÆ¡n (Ä‘Æ¡n vá»‹: VND) â€” cÃ ng nhá» cÃ ng tá»‘t.")
        st.caption("**MAPE**: % sai sá»‘ trung bÃ¬nh so vá»›i giÃ¡ tháº­t â€” cÃ ng nhá» cÃ ng tá»‘t.")

        st.markdown("### Biá»ƒu Ä‘á»“ (1 áº£nh / 1 hÃ ng)")
        plot_scatter(y_vl.values, st.session_state["y_pred_vl"])
        plot_residuals(y_vl.values, st.session_state["y_pred_vl"])
        st.markdown("#### Feature Importance")
        plot_feature_importance(st.session_state["trained_model"], features, top_k=15)

        st.divider()
        if st.button("ğŸ” ÄÃ¡nh giÃ¡ thÃªm trÃªn Test"):
            y_pred_ts = st.session_state["trained_model"].predict(X_ts)
            metrics_ts = calculate_metrics(y_ts.values, y_pred_ts)
            st.markdown("### Káº¿t quáº£ trÃªn Test")
            st.json(metrics_ts)
            st.session_state["test_pred"] = y_pred_ts
    else:
        st.info("Báº¥m **Báº¯t Ä‘áº§u huáº¥n luyá»‡n** Ä‘á»ƒ train model.")


with tab3:
    st.subheader("Dá»± Ä‘oÃ¡n báº±ng Upload CSV (khÃ´ng nháº­p tay)")
    if "trained_model" not in st.session_state:
        st.info("Báº¡n cáº§n huáº¥n luyá»‡n model trÆ°á»›c.")
    else:
        st.write("âœ… Upload 1 file CSV Ä‘á»ƒ dá»± Ä‘oÃ¡n. File cÃ³ thá»ƒ cÃ³ **1 dÃ²ng hoáº·c nhiá»u dÃ²ng**.")
        st.write("App sáº½ tá»± **align cá»™t** theo features lÃºc train (thá»«a cá»™t bá», thiáº¿u cá»™t â†’ NaN vÃ  imputer xá»­ lÃ½).")

        pred_file = st.file_uploader("Upload CSV Ä‘á»ƒ dá»± Ä‘oÃ¡n", type="csv", key="pred_csv")
        if pred_file:
            out_df = predict_from_csv(st.session_state["trained_model"], st.session_state["features"], pred_file)
            st.success("âœ… Dá»± Ä‘oÃ¡n xong!")
            st.dataframe(out_df.head(20), use_container_width=True)

            csv_bytes = out_df.to_csv(index=False).encode("utf-8")
            st.download_button("â¬‡ï¸ Táº£i file dá»± Ä‘oÃ¡n", csv_bytes, "predictions.csv", "text/csv")


with tab4:
    st.subheader("Xuáº¥t model vÃ  dá»± Ä‘oÃ¡n test")
    if "trained_model" not in st.session_state:
        st.info("Báº¡n cáº§n huáº¥n luyá»‡n model trÆ°á»›c.")
    else:
        joblib.dump(st.session_state["trained_model"], "model.joblib")
        with open("model.joblib", "rb") as f:
            st.download_button("ğŸ’¾ Táº£i model.joblib", f, "model.joblib")

        st.divider()

        if "test_pred" not in st.session_state:
            st.info("HÃ£y báº¥m **ÄÃ¡nh giÃ¡ thÃªm trÃªn Test** á»Ÿ tab Huáº¥n luyá»‡n Ä‘á»ƒ táº¡o file dá»± Ä‘oÃ¡n test.")
        else:
            test_results = df_ts.copy()
            test_results["predicted_price_base"] = st.session_state["test_pred"]
            csv = test_results.to_csv(index=False).encode("utf-8")
            st.download_button("ğŸ“Š Táº£i test_predictions.csv", csv, "test_predictions.csv", "text/csv")
