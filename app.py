import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import joblib
import os

from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from xgboost.callback import EarlyStopping as XGBoostEarlyStopping
from lightgbm import LGBMRegressor
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error


# =========================
# CONFIG
# =========================
st.set_page_config(page_title="·ª®ng d·ª•ng D·ª± ƒëo√°n Gi√° Laptop", layout="wide")


# =========================
# FUNCTIONS
# =========================
def load_data(train_file, val_file, test_file):
    df_train = pd.read_csv(train_file)
    df_val = pd.read_csv(val_file)
    df_test = pd.read_csv(test_file)
    return df_train, df_val, df_test


def align_columns(df_train, df_val, df_test, target="price_base"):
    # Drop title n·∫øu c√≥
    def _drop_title(df):
        if "title" in df.columns:
            return df.drop(columns=["title"])
        return df

    df_train = _drop_title(df_train)
    df_val = _drop_title(df_val)
    df_test = _drop_title(df_test)

    # intersection columns (tr√°nh test th·ª´a c·ªôt)
    common_cols = list(set(df_train.columns) & set(df_val.columns) & set(df_test.columns))

    if target not in df_train.columns or target not in df_val.columns or target not in df_test.columns:
        raise ValueError(f"Thi·∫øu c·ªôt target '{target}' trong 1 trong 3 t·∫≠p d·ªØ li·ªáu.")

    if target in common_cols:
        common_cols.remove(target)

    common_cols.sort()

    X_train = df_train[common_cols].copy()
    y_train = df_train[target].copy()

    X_val = df_val[common_cols].copy()
    y_val = df_val[target].copy()

    X_test = df_test[common_cols].copy()
    y_test = df_test[target].copy()

    return X_train, y_train, X_val, y_val, X_test, y_test, common_cols


def calculate_metrics(y_true, y_pred):
    r2 = float(r2_score(y_true, y_pred))
    mae = float(mean_absolute_error(y_true, y_pred))
    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))

    # Safe MAPE (tr√°nh chia 0 ho·∫∑c qu√° nh·ªè)
    eps = 1e-8
    denom = np.maximum(np.abs(y_true), eps)
    mape = float(np.mean(np.abs((y_true - y_pred) / denom)) * 100)

    # ‚úÖ ƒê√£ x√≥a Accuracy theo y√™u c·∫ßu
    return {"R2": r2, "MAE": mae, "RMSE": rmse, "MAPE (%)": mape}


def build_model(model_type, params):
    if model_type == "Random Forest":
        base_model = RandomForestRegressor(
            n_estimators=int(params["n_estimators"]),
            max_depth=None if int(params["max_depth"]) == 0 else int(params["max_depth"]),
            min_samples_split=int(params["min_samples_split"]),
            min_samples_leaf=int(params["min_samples_leaf"]),
            n_jobs=-1,
            random_state=42
        )
        use_early_stop = False

    elif model_type == "XGBoost":
        base_model = XGBRegressor(
            n_estimators=int(params["n_estimators"]),
            max_depth=int(params["max_depth"]),
            learning_rate=float(params["learning_rate"]),
            subsample=float(params["subsample"]),
            colsample_bytree=float(params["colsample_bytree"]),
            reg_alpha=float(params["reg_alpha"]),
            reg_lambda=float(params["reg_lambda"]),
            objective="reg:squarederror",
            eval_metric="rmse",
            n_jobs=-1,
            random_state=42
        )
        use_early_stop = True

    else:  # LightGBM
        base_model = LGBMRegressor(
            n_estimators=int(params["n_estimators"]),
            max_depth=-1 if int(params["max_depth"]) == 0 else int(params["max_depth"]),
            learning_rate=float(params["learning_rate"]),
            num_leaves=int(params["num_leaves"]),
            subsample=float(params["subsample"]),
            colsample_bytree=float(params["colsample_bytree"]),
            reg_alpha=float(params["reg_alpha"]),
            reg_lambda=float(params["reg_lambda"]),
            n_jobs=-1,
            random_state=42
        )
        use_early_stop = True

    pipeline = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("model", base_model)
    ])
    return pipeline, use_early_stop


def train_and_eval(model_pipeline, use_early_stop, model_type, X_tr, y_tr, X_vl, y_vl, early_rounds=50):
    if use_early_stop:
        X_tr_imp = model_pipeline.named_steps["imputer"].fit_transform(X_tr)
        X_vl_imp = model_pipeline.named_steps["imputer"].transform(X_vl)
        model = model_pipeline.named_steps["model"]

        if model_type == "XGBoost":
            # ‚úÖ FIX: d√πng callback EarlyStopping (·ªïn ƒë·ªãnh tr√™n Streamlit Cloud)
            cb = XGBoostEarlyStopping(rounds=int(early_rounds), save_best=True, maximize=False)
            model.fit(
                X_tr_imp, y_tr,
                eval_set=[(X_vl_imp, y_vl)],
                verbose=False,
                callbacks=[cb]
            )
            y_pred_vl = model.predict(X_vl_imp)

        else:  # LightGBM: train b√¨nh th∆∞·ªùng (nhanh), mu·ªën d·ª´ng s·ªõm th√¨ n√¢ng c·∫•p sau
            model.fit(
                X_tr_imp, y_tr,
                eval_set=[(X_vl_imp, y_vl)],
                eval_metric="l2",
            )
            y_pred_vl = model.predict(X_vl_imp)

        return model_pipeline, y_pred_vl

    # RandomForest
    model_pipeline.fit(X_tr, y_tr)
    y_pred_vl = model_pipeline.predict(X_vl)
    return model_pipeline, y_pred_vl


def plot_scatter(y_true, y_pred):
    fig = plt.figure(figsize=(7, 5))
    plt.scatter(y_true, y_pred, alpha=0.5)
    mn = float(min(np.min(y_true), np.min(y_pred)))
    mx = float(max(np.max(y_true), np.max(y_pred)))
    plt.plot([mn, mx], [mn, mx], "r--")
    plt.title("So s√°nh Gi√° Th·∫≠t vs Gi√° D·ª± ƒëo√°n (Validation)")
    plt.xlabel("Gi√° th·∫≠t")
    plt.ylabel("Gi√° d·ª± ƒëo√°n")
    st.pyplot(fig)


def plot_residuals(y_true, y_pred):
    residuals = y_true - y_pred
    fig = plt.figure(figsize=(7, 5))
    plt.hist(residuals, bins=30)
    plt.title("Ph√¢n ph·ªëi Sai s·ªë (Residuals) - Validation")
    plt.xlabel("Sai s·ªë (gi√° th·∫≠t - gi√° d·ª± ƒëo√°n)")
    plt.ylabel("S·ªë l∆∞·ª£ng")
    st.pyplot(fig)


def plot_feature_importance(model_pipeline, feature_names, top_k=15):
    raw_model = model_pipeline.named_steps["model"]
    if not hasattr(raw_model, "feature_importances_"):
        st.info("Model n√†y kh√¥ng h·ªó tr·ª£ Feature Importance.")
        return

    importances = raw_model.feature_importances_
    idx = np.argsort(importances)[-top_k:]
    names = [feature_names[i] for i in idx]
    vals = importances[idx]

    fig = plt.figure(figsize=(7, 6))
    plt.barh(names, vals)
    plt.title(f"Top {top_k} Feature Importance")
    plt.xlabel("M·ª©c ƒë·ªô quan tr·ªçng")
    st.pyplot(fig)


def predict_from_csv(trained_model, features, csv_file):
    df_in = pd.read_csv(csv_file)
    X_in = df_in.reindex(columns=features)
    preds = trained_model.predict(X_in)
    out = df_in.copy()
    out["predicted_price_base"] = preds
    return out


# =========================
# SIDEBAR (VN)
# =========================
st.sidebar.header("1) D·ªØ li·ªáu ƒë·∫ßu v√†o")
train_up = st.sidebar.file_uploader("Upload data_train.csv", type="csv")
val_up = st.sidebar.file_uploader("Upload data_validation.csv", type="csv")
test_up = st.sidebar.file_uploader("Upload data_test.csv", type="csv")

train_path = train_up if train_up else "data_train.csv"
val_path = val_up if val_up else "data_validation.csv"
test_path = test_up if test_up else "data_test.csv"

st.sidebar.header("2) Ch·ªçn m√¥ h√¨nh & tham s·ªë")
model_choice = st.sidebar.selectbox("M√¥ h√¨nh", ["Random Forest", "XGBoost", "LightGBM"])

fast_mode = st.sidebar.checkbox("‚ö° Hu·∫•n luy·ªán nhanh (khuy·∫øn ngh·ªã)", value=True)

# Ch·ªâ hi·ªÉn th·ªã early stopping khi ch·ªçn XGBoost
early_rounds = None
if model_choice == "XGBoost":
    early_rounds = st.sidebar.slider("D·ª´ng s·ªõm (Early stopping)", 10, 200, 50, 10)
    st.sidebar.caption(
        "T·ª± ƒë·ªông d·ª´ng hu·∫•n luy·ªán n·∫øu m√¥ h√¨nh kh√¥ng c·∫£i thi·ªán tr√™n Validation sau N v√≤ng. "
        "Gi√∫p train nhanh v√† tr√°nh overfitting."
    )

params = {}

# ===== Random Forest =====
if model_choice == "Random Forest":
    params["n_estimators"] = st.sidebar.slider(
        "S·ªë l∆∞·ª£ng c√¢y (n_estimators)", 50, 600, 200 if fast_mode else 400, 50
    )
    st.sidebar.caption("S·ªë c√¢y c√†ng nhi·ªÅu ‚Üí th∆∞·ªùng t·ªët h∆°n nh∆∞ng train ch·∫≠m h∆°n. G·ª£i √Ω: 200‚Äì500.")

    params["max_depth"] = st.sidebar.slider(
        "ƒê·ªô s√¢u t·ªëi ƒëa c·ªßa c√¢y (max_depth) - 0 = kh√¥ng gi·ªõi h·∫°n", 0, 30, 0 if fast_mode else 12, 1
    )
    st.sidebar.caption("ƒê·ªô s√¢u l·ªõn ‚Üí m√¥ h√¨nh ph·ª©c t·∫°p h∆°n (d·ªÖ overfit). G·ª£i √Ω: 8‚Äì16 ho·∫∑c 0 n·∫øu mu·ªën th·ª≠.")

    params["min_samples_split"] = st.sidebar.slider(
        "S·ªë m·∫´u t·ªëi thi·ªÉu ƒë·ªÉ t√°ch nh√°nh (min_samples_split)", 2, 20, 2, 1
    )
    st.sidebar.caption("TƒÉng gi√° tr·ªã n√†y ‚Üí c√¢y ‚Äòkh√≥ t√°ch‚Äô h∆°n ‚Üí gi·∫£m overfit.")

    params["min_samples_leaf"] = st.sidebar.slider(
        "S·ªë m·∫´u t·ªëi thi·ªÉu trong 1 l√° (min_samples_leaf)", 1, 20, 1, 1
    )
    st.sidebar.caption("TƒÉng gi√° tr·ªã n√†y ‚Üí m√¥ h√¨nh ·ªïn ƒë·ªãnh h∆°n, nh∆∞ng c√≥ th·ªÉ gi·∫£m ƒë·ªô ch√≠nh x√°c.")

# ===== XGBoost =====
elif model_choice == "XGBoost":
    params["n_estimators"] = st.sidebar.slider(
        "S·ªë v√≤ng boosting (n_estimators)", 200, 2500, 600 if fast_mode else 1500, 100
    )
    st.sidebar.caption("Gi·ªëng ‚Äòs·ªë c√¢y / s·ªë v√≤ng‚Äô tƒÉng d·∫ßn. Nhi·ªÅu qu√° s·∫Ω ch·∫≠m; d√πng Early stopping ƒë·ªÉ d·ª´ng s·ªõm.")

    params["max_depth"] = st.sidebar.slider("ƒê·ªô s√¢u c√¢y (max_depth)", 2, 12, 6 if fast_mode else 8, 1)
    st.sidebar.caption("ƒê·ªô s√¢u l·ªõn ‚Üí b·∫Øt pattern m·∫°nh h∆°n nh∆∞ng d·ªÖ overfit. G·ª£i √Ω: 4‚Äì8.")

    params["learning_rate"] = st.sidebar.number_input(
        "T·ªëc ƒë·ªô h·ªçc (learning_rate)", 0.005, 0.3, 0.05 if fast_mode else 0.03, step=0.005
    )
    st.sidebar.caption("Learning rate nh·ªè ‚Üí c·∫ßn nhi·ªÅu v√≤ng h∆°n nh∆∞ng ·ªïn ƒë·ªãnh h∆°n. G·ª£i √Ω: 0.03‚Äì0.1.")

    params["subsample"] = st.sidebar.slider("T·ªâ l·ªá l·∫•y m·∫´u d·ªØ li·ªáu (subsample)", 0.5, 1.0, 0.9, 0.05)
    st.sidebar.caption("Gi·∫£m <1.0 gi√∫p ch·ªëng overfit (l·∫•y ng·∫´u nhi√™n 1 ph·∫ßn d·ªØ li·ªáu m·ªói v√≤ng).")

    params["colsample_bytree"] = st.sidebar.slider("T·ªâ l·ªá l·∫•y m·∫´u feature (colsample_bytree)", 0.5, 1.0, 0.9, 0.05)
    st.sidebar.caption("Gi·∫£m <1.0 gi√∫p ch·ªëng overfit (m·ªói c√¢y ch·ªâ d√πng m·ªôt ph·∫ßn feature).")

    params["reg_alpha"] = st.sidebar.number_input("Regularization L1 (reg_alpha)", 0.0, 10.0, 0.0, step=0.1)
    st.sidebar.caption("TƒÉng l√™n n·∫øu b·∫°n th·∫•y overfit ho·∫∑c feature nhi·ªÖu nhi·ªÅu.")

    params["reg_lambda"] = st.sidebar.number_input("Regularization L2 (reg_lambda)", 0.0, 10.0, 1.0, step=0.1)
    st.sidebar.caption("Th∆∞·ªùng ƒë·ªÉ ~1.0. TƒÉng l√™n n·∫øu mu·ªën m√¥ h√¨nh ‚Äòm∆∞·ª£t‚Äô h∆°n.")

# ===== LightGBM =====
else:
    params["n_estimators"] = st.sidebar.slider(
        "S·ªë v√≤ng boosting (n_estimators)", 200, 5000, 800 if fast_mode else 2500, 100
    )
    st.sidebar.caption("Nhi·ªÅu v√≤ng h∆°n ‚Üí c√≥ th·ªÉ t·ªët h∆°n nh∆∞ng ch·∫≠m h∆°n. G·ª£i √Ω: 800‚Äì2500.")

    params["max_depth"] = st.sidebar.slider(
        "ƒê·ªô s√¢u t·ªëi ƒëa (max_depth) - 0 = kh√¥ng gi·ªõi h·∫°n", 0, 30, 0 if fast_mode else 10, 1
    )
    st.sidebar.caption("Gi·ªõi h·∫°n depth ƒë·ªÉ tr√°nh overfit. G·ª£i √Ω: 6‚Äì12 ho·∫∑c 0 n·∫øu mu·ªën th·ª≠.")

    params["learning_rate"] = st.sidebar.number_input(
        "T·ªëc ƒë·ªô h·ªçc (learning_rate)", 0.005, 0.3, 0.05 if fast_mode else 0.03, step=0.005
    )
    st.sidebar.caption("Nh·ªè h∆°n ‚Üí ·ªïn ƒë·ªãnh h∆°n nh∆∞ng c·∫ßn nhi·ªÅu v√≤ng h∆°n. G·ª£i √Ω: 0.03‚Äì0.1.")

    params["num_leaves"] = st.sidebar.slider("S·ªë l√° t·ªëi ƒëa (num_leaves)", 15, 127, 31 if fast_mode else 63, 2)
    st.sidebar.caption("num_leaves l·ªõn ‚Üí m√¥ h√¨nh m·∫°nh h∆°n nh∆∞ng d·ªÖ overfit. G·ª£i √Ω: 31‚Äì63.")

    params["subsample"] = st.sidebar.slider("T·ªâ l·ªá l·∫•y m·∫´u d·ªØ li·ªáu (subsample)", 0.5, 1.0, 0.9, 0.05)
    st.sidebar.caption("Gi·∫£m <1.0 gi√∫p ch·ªëng overfit.")

    params["colsample_bytree"] = st.sidebar.slider("T·ªâ l·ªá l·∫•y m·∫´u feature (colsample_bytree)", 0.5, 1.0, 0.9, 0.05)
    st.sidebar.caption("Gi·∫£m <1.0 gi√∫p ch·ªëng overfit.")

    params["reg_alpha"] = st.sidebar.number_input("Regularization L1 (reg_alpha)", 0.0, 10.0, 0.0, step=0.1)
    st.sidebar.caption("TƒÉng n·∫øu d·ªØ li·ªáu nhi·ªÖu ho·∫∑c overfit.")

    params["reg_lambda"] = st.sidebar.number_input("Regularization L2 (reg_lambda)", 0.0, 10.0, 0.0, step=0.1)
    st.sidebar.caption("TƒÉng n·∫øu mu·ªën m√¥ h√¨nh ·ªïn ƒë·ªãnh h∆°n.")


# =========================
# MAIN
# =========================
st.title("üíª ·ª®ng d·ª•ng D·ª± ƒëo√°n Gi√° Laptop")

def _available(p):
    return (hasattr(p, "read")) or os.path.exists(str(p))

if not all(_available(p) for p in [train_path, val_path, test_path]):
    st.info("Vui l√≤ng ƒë·∫∑t ƒë·ªß 3 file: data_train.csv, data_validation.csv, data_test.csv (ho·∫∑c upload ·ªü sidebar).")
    st.stop()

try:
    df_tr, df_vl, df_ts = load_data(train_path, val_path, test_path)
    X_tr, y_tr, X_vl, y_vl, X_ts, y_ts, features = align_columns(df_tr, df_vl, df_ts)
except Exception as e:
    st.error(f"L·ªói x·ª≠ l√Ω d·ªØ li·ªáu: {e}")
    st.stop()

tab1, tab2, tab3, tab4 = st.tabs(["üìå Xem d·ªØ li·ªáu", "üß† Hu·∫•n luy·ªán & ƒê√°nh gi√°", "üéØ D·ª± ƒëo√°n", "üì§ Xu·∫•t file"])


with tab1:
    st.subheader("Xem nhanh d·ªØ li·ªáu (head)")
    c1, c2, c3 = st.columns(3)
    with c1:
        st.markdown("### Train")
        st.write("K√≠ch th∆∞·ªõc:", df_tr.shape)
        st.dataframe(df_tr.head(10), use_container_width=True)
    with c2:
        st.markdown("### Validation")
        st.write("K√≠ch th∆∞·ªõc:", df_vl.shape)
        st.dataframe(df_vl.head(10), use_container_width=True)
    with c3:
        st.markdown("### Test")
        st.write("K√≠ch th∆∞·ªõc:", df_ts.shape)
        st.dataframe(df_ts.head(10), use_container_width=True)

    st.info(f"ƒê√£ l·∫•y **c·ªôt chung (intersection)** gi·ªØa train/val/test: **{len(features)} features** (ƒë√£ lo·∫°i 'title' v√† target).")


with tab2:
    st.subheader("Hu·∫•n luy·ªán m√¥ h√¨nh")
    start_train = st.button("üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán", type="primary")

    if start_train:
        with st.spinner("ƒêang hu·∫•n luy·ªán..."):
            model_pipeline, use_es = build_model(model_choice, params)
            model_pipeline, y_pred_vl = train_and_eval(
                model_pipeline, use_es, model_choice, X_tr, y_tr, X_vl, y_vl,
                early_rounds=(early_rounds if early_rounds is not None else 50)
            )

            st.session_state["trained_model"] = model_pipeline
            st.session_state["features"] = features
            st.session_state["y_pred_vl"] = y_pred_vl

            metrics_vl = calculate_metrics(y_vl.values, y_pred_vl)
            st.session_state["metrics_vl"] = metrics_vl

        st.success("‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t!")

    if "trained_model" in st.session_state:
        st.markdown("### K·∫øt qu·∫£ tr√™n Validation")
        metrics_vl = st.session_state["metrics_vl"]

        m_cols = st.columns(4)
        keys = list(metrics_vl.keys())
        for i, k in enumerate(keys):
            m_cols[i].metric(k, f"{metrics_vl[k]:,.2f}")

        st.markdown("### Bi·ªÉu ƒë·ªì (1 ·∫£nh / 1 h√†ng)")
        plot_scatter(y_vl.values, st.session_state["y_pred_vl"])
        plot_residuals(y_vl.values, st.session_state["y_pred_vl"])
        st.markdown("#### Feature Importance")
        plot_feature_importance(st.session_state["trained_model"], features, top_k=15)

        st.divider()
        if st.button("üîç ƒê√°nh gi√° th√™m tr√™n Test"):
            y_pred_ts = st.session_state["trained_model"].predict(X_ts)
            metrics_ts = calculate_metrics(y_ts.values, y_pred_ts)
            st.markdown("### K·∫øt qu·∫£ tr√™n Test")
            st.json(metrics_ts)
            st.session_state["test_pred"] = y_pred_ts
    else:
        st.info("B·∫•m **B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán** ƒë·ªÉ train model.")


with tab3:
    st.subheader("D·ª± ƒëo√°n b·∫±ng Upload CSV (kh√¥ng nh·∫≠p tay)")
    if "trained_model" not in st.session_state:
        st.info("B·∫°n c·∫ßn hu·∫•n luy·ªán model tr∆∞·ªõc.")
    else:
        st.write("‚úÖ Upload 1 file CSV ƒë·ªÉ d·ª± ƒëo√°n. File c√≥ th·ªÉ c√≥ **1 d√≤ng ho·∫∑c nhi·ªÅu d√≤ng**.")
        st.write("App s·∫Ω t·ª± **align c·ªôt** theo features l√∫c train (th·ª´a c·ªôt b·ªè, thi·∫øu c·ªôt ‚Üí NaN v√† imputer x·ª≠ l√Ω).")

        pred_file = st.file_uploader("Upload CSV ƒë·ªÉ d·ª± ƒëo√°n", type="csv", key="pred_csv")
        if pred_file:
            out_df = predict_from_csv(st.session_state["trained_model"], st.session_state["features"], pred_file)
            st.success("‚úÖ D·ª± ƒëo√°n xong!")
            st.dataframe(out_df.head(20), use_container_width=True)

            csv_bytes = out_df.to_csv(index=False).encode("utf-8")
            st.download_button("‚¨áÔ∏è T·∫£i file d·ª± ƒëo√°n", csv_bytes, "predictions.csv", "text/csv")


with tab4:
    st.subheader("Xu·∫•t model v√† d·ª± ƒëo√°n test")
    if "trained_model" not in st.session_state:
        st.info("B·∫°n c·∫ßn hu·∫•n luy·ªán model tr∆∞·ªõc.")
    else:
        joblib.dump(st.session_state["trained_model"], "model.joblib")
        with open("model.joblib", "rb") as f:
            st.download_button("üíæ T·∫£i model.joblib", f, "model.joblib")

        st.divider()

        if "test_pred" not in st.session_state:
            st.info("H√£y b·∫•m **ƒê√°nh gi√° th√™m tr√™n Test** ·ªü tab Hu·∫•n luy·ªán ƒë·ªÉ t·∫°o file d·ª± ƒëo√°n test.")
        else:
            test_results = df_ts.copy()
            test_results["predicted_price_base"] = st.session_state["test_pred"]
            csv = test_results.to_csv(index=False).encode("utf-8")
            st.download_button("üìä T·∫£i test_predictions.csv", csv, "test_predictions.csv", "text/csv")
