import os
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance

# XGBoost notebook
from xgboost import XGBRegressor
from sklearn.preprocessing import OrdinalEncoder

# "LightGBM" notebook th·ª±c ra d√πng HistGradientBoostingRegressor
from sklearn.ensemble import HistGradientBoostingRegressor


# =========================
# CONFIG
# =========================
st.set_page_config(page_title="·ª®ng d·ª•ng D·ª± ƒëo√°n Gi√° Laptop", layout="wide")

# =========================
# HELPERS
# =========================
def read_csv_fallback(file_or_path):
    """Read CSV with encoding fallback."""
    if file_or_path is None:
        return None
    try:
        return pd.read_csv(file_or_path, encoding="latin-1")
    except Exception:
        return pd.read_csv(file_or_path, encoding="utf-8")


def clean_currency_xgb(x):
    """Notebook model_XGBoost.ipynb"""
    if isinstance(x, str):
        clean_str = "".join(filter(str.isdigit, x))
        try:
            return float(clean_str)
        except ValueError:
            return np.nan
    return x


def clean_price_hgb(x):
    """Notebook Tu_LightGBM.ipynb"""
    if pd.isna(x):
        return np.nan
    s = str(x).replace(".", "").replace(",", "").replace("ƒë", "").strip()
    try:
        val = float(s)
        return val if 1e6 < val < 500e6 else np.nan
    except Exception:
        return np.nan


def calc_metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))
    r2 = r2_score(y_true, y_pred)
    # MAPE (%)
    eps = 1e-9
    mape = float(np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + eps))) * 100.0)
    return r2, mae, rmse, mape


def make_results_df(rows):
    return pd.DataFrame(rows, columns=["Dataset", "R2", "MAE", "RMSE", "MAPE (%)"])


def style_small_ticks(ax):
    ax.tick_params(axis="both", labelsize=8)


# =========================
# PREPROCESS: RANDOM FOREST (GI·ªÆ NGUY√äN T∆Ø DUY PIPELINE)
# =========================
def prepare_rf_data(df_train, df_val, df_test, target="price_base"):
    # drop title n·∫øu c√≥
    for df in (df_train, df_val, df_test):
        if df is not None and "title" in df.columns:
            df.drop(columns=["title"], inplace=True, errors="ignore")

    # ch·ªâ l·∫•y c·ªôt chung
    common_cols = set(df_train.columns) & set(df_val.columns) & set(df_test.columns)
    common_cols = list(common_cols)

    if target not in common_cols:
        # target c√≥ th·ªÉ t·ªìn t·∫°i nh∆∞ng kh√¥ng n·∫±m trong common_cols do df n√†o ƒë√≥ thi·∫øu -> √©p ƒë·∫£m b·∫£o
        if target in df_train.columns and target in df_val.columns and target in df_test.columns:
            pass
        else:
            raise ValueError("Kh√¥ng t√¨m th·∫•y c·ªôt target 'price_base' trong ƒë·ªß 3 file.")

    # features: t·∫•t c·∫£ tr·ª´ target
    feature_cols = [c for c in common_cols if c != target]
    # lo·∫°i th√™m v√†i c·ªôt ƒë·ªãnh danh n·∫øu c√≥
    for bad in ["link", "url", "id", "price_sale"]:
        if bad in feature_cols:
            feature_cols.remove(bad)

    X_train = df_train[feature_cols].copy()
    y_train = pd.to_numeric(df_train[target], errors="coerce")

    X_val = df_val[feature_cols].copy()
    y_val = pd.to_numeric(df_val[target], errors="coerce")

    X_test = df_test[feature_cols].copy()
    y_test = pd.to_numeric(df_test[target], errors="coerce")

    # keep only numeric for RF pipeline (n·∫øu d·ªØ li·ªáu b·∫°n to√†n numeric th√¨ OK)
    # N·∫øu c√≥ object l·∫´n v√†o -> c·ªë √©p numeric
    for X in (X_train, X_val, X_test):
        for c in X.columns:
            X[c] = pd.to_numeric(X[c], errors="coerce")

    # drop nan y
    mask_tr = y_train.notna()
    mask_vl = y_val.notna()
    mask_ts = y_test.notna()

    return (
        X_train.loc[mask_tr], y_train.loc[mask_tr].astype(float).values,
        X_val.loc[mask_vl], y_val.loc[mask_vl].astype(float).values,
        X_test.loc[mask_ts], y_test.loc[mask_ts].astype(float).values,
        feature_cols
    )


# =========================
# PREPROCESS: XGBOOST (THEO model_XGBoost.ipynb)
# =========================
def prepare_xgb_data(df_train, df_val, df_test):
    # clean price_base
    for df in (df_train, df_val, df_test):
        if "price_base" in df.columns:
            df["price_base"] = df["price_base"].apply(clean_currency_xgb)

    # filter price too small
    df_train = df_train[df_train["price_base"] > 1_000_000].copy()
    df_val = df_val[df_val["price_base"] > 1_000_000].copy()
    df_test = df_test[df_test["price_base"] > 1_000_000].copy()

    exclude_cols = ["title", "price_base", "price_sale", "link", "url", "id"]
    feature_cols = [c for c in df_train.columns if c not in exclude_cols]

    # align columns for val/test
    for col in feature_cols:
        if col not in df_val.columns:
            df_val[col] = 0
        if col not in df_test.columns:
            df_test[col] = 0

    # numeric/categorical
    numeric_cols = df_train[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = df_train[feature_cols].select_dtypes(include=["object", "category"]).columns.tolist()

    # categorical preprocess like notebook
    ord_enc = None
    if len(categorical_cols) > 0:
        for df in (df_train, df_val, df_test):
            df[categorical_cols] = df[categorical_cols].fillna("Unknown")

        ord_enc = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)
        df_train[categorical_cols] = ord_enc.fit_transform(df_train[categorical_cols].astype(str))
        df_val[categorical_cols] = ord_enc.transform(df_val[categorical_cols].astype(str))
        df_test[categorical_cols] = ord_enc.transform(df_test[categorical_cols].astype(str))

        # cast int -> category (xgb enable_categorical)
        for col in categorical_cols:
            df_train[col] = df_train[col].astype(int).astype("category")
            df_val[col] = df_val[col].astype(int).astype("category")
            df_test[col] = df_test[col].astype(int).astype("category")

    # numeric impute median (ƒë·ªÉ gi·ªëng pipeline)
    for df in (df_train, df_val, df_test):
        for c in numeric_cols:
            df[c] = pd.to_numeric(df[c], errors="coerce")
            med = df_train[c].median()
            df[c] = df[c].fillna(med)

    X_train = df_train[feature_cols]
    y_train = np.log1p(df_train["price_base"].values)

    X_val = df_val[feature_cols]
    y_val = np.log1p(df_val["price_base"].values)

    X_test = df_test[feature_cols]
    y_test = df_test["price_base"].values.astype(float)

    return X_train, y_train, X_val, y_val, X_test, y_test, feature_cols


# =========================
# PREPROCESS: HGB (THEO Tu_LightGBM.ipynb)
# =========================
def prepare_hgb_data(df_train, df_val, df_test):
    for df in (df_train, df_val, df_test):
        df["price_base"] = df["price_base"].apply(clean_price_hgb)
        df.dropna(subset=["price_base"], inplace=True)

    # collect numeric cols across all
    all_numeric_cols = set()
    for df in (df_train, df_val, df_test):
        all_numeric_cols.update(df.select_dtypes(include=[np.number]).columns)

    target_related = {"price_base", "price_sale"}
    feats = sorted(list(all_numeric_cols - target_related))

    # storage_type -> is_ssd
    def map_ssd(x):
        s = str(x).upper()
        if s == "SSD":
            return 1.0
        if s == "HDD":
            return 0.0
        return 0.5

    for df in (df_train, df_val, df_test):
        if "storage_type" in df.columns:
            df["is_ssd"] = df["storage_type"].apply(map_ssd)
        else:
            df["is_ssd"] = 0.5

    feats = sorted(list(set(feats + ["is_ssd"])))

    def finalize(df, train_ref=None):
        X = pd.DataFrame(index=df.index)
        for c in feats:
            if c in df.columns:
                X[c] = pd.to_numeric(df[c], errors="coerce")
            else:
                X[c] = 0.0

        for c in feats:
            ref = train_ref if train_ref is not None else df
            if c in ref.columns:
                m = pd.to_numeric(ref[c], errors="coerce").median()
            else:
                m = 0.0
            if pd.isna(m):
                m = 0.0
            X[c] = X[c].fillna(m)

        y_log = np.log1p(df["price_base"].values.astype(float))
        y_raw = df["price_base"].values.astype(float)
        return X, y_log, y_raw

    X_tr, y_tr_l, y_tr = finalize(df_train, None)
    X_vl, y_vl_l, y_vl = finalize(df_val, df_train)
    X_ts, y_ts_l, y_ts = finalize(df_test, df_train)

    return X_tr, y_tr_l, y_tr, X_vl, y_vl_l, y_vl, X_ts, y_ts, feats


# =========================
# UI
# =========================
st.title("üíª ·ª®ng d·ª•ng D·ª± ƒëo√°n Gi√° Laptop")

with st.sidebar:
    st.header("1) Upload d·ªØ li·ªáu")
    up_train = st.file_uploader("Upload data_train.csv", type=["csv"])
    up_val = st.file_uploader("Upload data_validation.csv", type=["csv"])
    up_test = st.file_uploader("Upload data_test.csv", type=["csv"])

    st.header("2) Ch·ªçn m√¥ h√¨nh")
    model_name = st.selectbox("M√¥ h√¨nh", ["Random Forest", "XGBoost", "LightGBM (theo notebook)"])

    # ===== Random Forest sliders =====
    rf_params = {}
    if model_name == "Random Forest":
        rf_params["n_estimators"] = st.slider(
            "n_estimators",
            50, 2000, 500, 50,
            help="S·ªë c√¢y. TƒÉng th∆∞·ªùng ·ªïn ƒë·ªãnh h∆°n nh∆∞ng train ch·∫≠m h∆°n."
        )
        rf_params["max_depth"] = st.slider(
            "max_depth (0 = None)",
            0, 40, 0, 1,
            help="ƒê·ªô s√¢u t·ªëi ƒëa c·ªßa c√¢y. TƒÉng -> fit m·∫°nh h∆°n nh∆∞ng d·ªÖ overfit. Gi·∫£m -> t·ªïng qu√°t t·ªët h∆°n."
        )
        rf_params["min_samples_split"] = st.slider(
            "min_samples_split",
            2, 30, 2, 1,
            help="S·ªë m·∫´u t·ªëi thi·ªÉu ƒë·ªÉ t√°ch node. TƒÉng -> gi·∫£m overfit, nh∆∞ng c√≥ th·ªÉ underfit."
        )
        rf_params["min_samples_leaf"] = st.slider(
            "min_samples_leaf",
            1, 30, 1, 1,
            help="S·ªë m·∫´u t·ªëi thi·ªÉu ·ªü l√°. TƒÉng -> m√¥ h√¨nh m∆∞·ª£t h∆°n, th∆∞·ªùng gi√∫p Test R2 t·ªët h∆°n n·∫øu ƒëang overfit."
        )
        rf_params["max_features"] = st.slider(
            "max_features",
            0.10, 1.0, 0.70, 0.05,
            help="T·ªâ l·ªá feature d√πng m·ªói l·∫ßn split. Gi·∫£m -> tƒÉng ƒëa d·∫°ng c√¢y, gi·∫£m overfit (th∆∞·ªùng t·ªët cho test)."
        )
        rf_params["random_state"] = 42
        rf_params["n_jobs"] = -1

    st.divider()
    st.caption(
        "G·ª£i √Ω nhanh: n·∫øu Train R2 cao nh∆∞ng Test R2 th·∫•p ‚Üí ƒëang **overfit**. "
        "H√£y **gi·∫£m max_depth**, **tƒÉng min_samples_leaf**, v√† **gi·∫£m max_features**."
    )

train_btn = st.button("üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán")


# =========================
# LOAD DATA
# =========================
def load_default_or_upload(uploaded, default_name):
    if uploaded is not None:
        return read_csv_fallback(uploaded)
    if os.path.exists(default_name):
        return read_csv_fallback(default_name)
    return None


df_train = load_default_or_upload(up_train, "data_train.csv")
df_val = load_default_or_upload(up_val, "data_validation.csv")
df_test = load_default_or_upload(up_test, "data_test.csv")

if df_train is None or df_val is None or df_test is None:
    st.info("H√£y upload ƒë·ªß 3 file ho·∫∑c ƒë·∫∑t 3 file c√πng th∆∞ m·ª•c v·ªõi app.py: data_train.csv, data_validation.csv, data_test.csv")
    st.stop()


# =========================
# TRAIN
# =========================
if train_btn:
    st.success("‚úÖ ƒêang hu·∫•n luy·ªán...")

    results_rows = []
    fig_scatter = None
    fig_resid = None
    fig_imp = None

    # -------------------------
    # RANDOM FOREST
    # -------------------------
    if model_name == "Random Forest":
        X_tr, y_tr, X_vl, y_vl, X_ts, y_ts, feature_cols = prepare_rf_data(df_train.copy(), df_val.copy(), df_test.copy())

        pipeline = Pipeline([
            ("imputer", SimpleImputer(strategy="median")),
            ("model", RandomForestRegressor(
                n_estimators=rf_params["n_estimators"],
                max_depth=None if rf_params["max_depth"] == 0 else rf_params["max_depth"],
                min_samples_split=rf_params["min_samples_split"],
                min_samples_leaf=rf_params["min_samples_leaf"],
                max_features=rf_params["max_features"],
                random_state=rf_params["random_state"],
                n_jobs=rf_params["n_jobs"],
            ))
        ])

        pipeline.fit(X_tr, y_tr)

        # Predict
        pred_tr = pipeline.predict(X_tr)
        pred_vl = pipeline.predict(X_vl)
        pred_ts = pipeline.predict(X_ts)

        # Metrics
        for name, yt, yp in [
            ("Train", y_tr, pred_tr),
            ("Validation", y_vl, pred_vl),
            ("Test", y_ts, pred_ts),
        ]:
            r2, mae, rmse, mape = calc_metrics(yt, yp)
            results_rows.append([name, r2, mae, rmse, mape])

        # Visualize: use TEST
        y_true = y_ts
        y_pred = pred_ts
        resid = y_true - y_pred

        # Scatter
        fig_scatter, ax = plt.subplots(figsize=(7, 4))
        ax.scatter(y_true, y_pred, alpha=0.35)
        mn = float(min(y_true.min(), y_pred.min()))
        mx = float(max(y_true.max(), y_pred.max()))
        ax.plot([mn, mx], [mn, mx], linestyle="--")
        ax.set_title("Actual vs Predicted Prices (Test)")
        ax.set_xlabel("Actual Price (VND)")
        ax.set_ylabel("Predicted Price (VND)")
        style_small_ticks(ax)

        # Residuals
        fig_resid, ax2 = plt.subplots(figsize=(7, 4))
        ax2.hist(resid, bins=40, alpha=0.8)
        ax2.axvline(0, linestyle="--")
        ax2.set_title("Residuals Distribution (Sai s·ªë - Test)")
        ax2.set_xlabel("Error Amount (VND)")
        ax2.set_ylabel("Count")
        style_small_ticks(ax2)

        # Feature importance (RF)
        model = pipeline.named_steps["model"]
        importances = getattr(model, "feature_importances_", None)

        top_k = 15
        if importances is not None:
            imp = pd.Series(importances, index=feature_cols).sort_values(ascending=False).head(top_k)[::-1]
            fig_imp, ax3 = plt.subplots(figsize=(7, 4.5))
            ax3.barh(imp.index, imp.values)
            ax3.set_title(f"Top {top_k} Feature Importances (Random Forest)")
            ax3.set_xlabel("Relative Importance")
            style_small_ticks(ax3)

        # G·ª£i √Ω params (d·ª±a gap)
        r2_train = results_rows[0][1]
        r2_test = results_rows[2][1]
        gap = r2_train - r2_test

        with st.expander("üìå G·ª£i √Ω th√¥ng s·ªë ƒë·ªÉ Test R2 t·ªët h∆°n"):
            if gap > 0.12:
                st.warning(
                    f"Train R2 ({r2_train:.3f}) cao h∆°n Test R2 ({r2_test:.3f}) kh√° nhi·ªÅu ‚Üí **overfit**."
                )
                st.markdown(
                    "- Gi·∫£m `max_depth` (v√≠ d·ª• 12‚Äì20)\n"
                    "- TƒÉng `min_samples_leaf` (v√≠ d·ª• 3‚Äì8)\n"
                    "- TƒÉng `min_samples_split` (v√≠ d·ª• 6‚Äì15)\n"
                    "- Gi·∫£m `max_features` (v√≠ d·ª• 0.4‚Äì0.7)\n"
                    "- (N·∫øu b·∫°n th√™m ƒë∆∞·ª£c) `max_samples` ~ 0.7‚Äì0.9 ƒë·ªÉ gi·∫£m overfit\n"
                )
            else:
                st.success("Gap Train‚ÄìTest kh√¥ng qu√° l·ªõn. B·∫°n c√≥ th·ªÉ th·ª≠ tƒÉng nh·∫π n_estimators v√† t·ªëi ∆∞u max_depth.")

            st.caption("Preset hay ·ªïn ƒë·ªãnh cho laptop price (RF): n_estimators=800‚Äì1200, max_depth=16, min_samples_leaf=4, min_samples_split=10, max_features=0.5‚Äì0.7")

    # -------------------------
    # XGBOOST (theo model_XGBoost.ipynb)
    # -------------------------
    elif model_name == "XGBoost":
        X_tr, y_tr_l, X_vl, y_vl_l, X_ts, y_ts, feature_cols = prepare_xgb_data(df_train.copy(), df_val.copy(), df_test.copy())

        model = XGBRegressor(
            n_estimators=2000,
            learning_rate=0.01,

            max_depth=4,
            min_child_weight=5,
            gamma=0.2,

            subsample=0.6,
            colsample_bytree=0.6,

            reg_alpha=1.0,
            reg_lambda=2.0,

            objective="reg:squarederror",
            tree_method="hist",
            enable_categorical=True,
            n_jobs=-1,
            random_state=42,
            early_stopping_rounds=100,
            eval_metric="rmse"
        )

        model.fit(
            X_tr, y_tr_l,
            eval_set=[(X_tr, y_tr_l), (X_vl, y_vl_l)],
            verbose=False
        )

        # preds: train/val tr√™n log ‚Üí convert v·ªÅ VND ƒë·ªÉ t√≠nh metrics gi·ªëng b·∫£ng b·∫°n
        pred_tr = np.expm1(model.predict(X_tr))
        pred_vl = np.expm1(model.predict(X_vl))
        pred_ts = np.expm1(model.predict(X_ts))

        y_tr = np.expm1(y_tr_l)
        y_vl = np.expm1(y_vl_l)

        for name, yt, yp in [
            ("Train", y_tr, pred_tr),
            ("Validation", y_vl, pred_vl),
            ("Test", y_ts, pred_ts),
        ]:
            r2, mae, rmse, mape = calc_metrics(yt, yp)
            results_rows.append([name, r2, mae, rmse, mape])

        # Visualize: TEST
        y_true = y_ts
        y_pred = pred_ts
        resid = y_true - y_pred

        fig_scatter, ax = plt.subplots(figsize=(7, 4))
        ax.scatter(y_true, y_pred, alpha=0.35)
        mn = float(min(y_true.min(), y_pred.min()))
        mx = float(max(y_true.max(), y_pred.max()))
        ax.plot([mn, mx], [mn, mx], linestyle="--")
        ax.set_title("Actual vs Predicted Prices (Test)")
        ax.set_xlabel("Actual Price (VND)")
        ax.set_ylabel("Predicted Price (VND)")
        style_small_ticks(ax)

        fig_resid, ax2 = plt.subplots(figsize=(7, 4))
        ax2.hist(resid, bins=40, alpha=0.8)
        ax2.axvline(0, linestyle="--")
        ax2.set_title("Residuals Distribution (Sai s·ªë - Test)")
        ax2.set_xlabel("Error Amount (VND)")
        ax2.set_ylabel("Count")
        style_small_ticks(ax2)

        # Feature importance (XGB built-in)
        top_k = 15
        importances = getattr(model, "feature_importances_", None)
        if importances is not None:
            imp = pd.Series(importances, index=feature_cols).sort_values(ascending=False).head(top_k)[::-1]
            fig_imp, ax3 = plt.subplots(figsize=(7, 4.5))
            ax3.barh(imp.index, imp.values)
            ax3.set_title(f"Top {top_k} Feature Importances (XGBoost)")
            ax3.set_xlabel("Relative Importance")
            style_small_ticks(ax3)

        with st.expander("üìå G·ª£i √Ω th√¥ng s·ªë ƒë·ªÉ Test R2 t·ªët h∆°n (XGBoost)"):
            st.markdown(
                "- N·∫øu overfit: gi·∫£m `max_depth` (3‚Äì4), tƒÉng `min_child_weight` (5‚Äì10), tƒÉng `reg_lambda` (2‚Äì5)\n"
                "- N·∫øu underfit: tƒÉng `max_depth` (5‚Äì6) ho·∫∑c tƒÉng `n_estimators` (nh∆∞ng gi·ªØ `learning_rate` nh·ªè)\n"
                "- Th∆∞·ªùng ·ªïn ƒë·ªãnh: `subsample/colsample_bytree` trong 0.6‚Äì0.9\n"
            )
            st.caption("Notebook preset b·∫°n g·ª≠i ƒëang thi√™n v·ªÅ ch·ªëng overfit, kh√° h·ª£p n·∫øu d·ªØ li·ªáu nhi·ªÅu nhi·ªÖu.")

    # -------------------------
    # "LIGHTGBM" theo Tu_LightGBM.ipynb (HGB)
    # -------------------------
    else:
        X_tr, y_tr_l, y_tr, X_vl, y_vl_l, y_vl, X_ts, y_ts, feats = prepare_hgb_data(df_train.copy(), df_val.copy(), df_test.copy())

        model = HistGradientBoostingRegressor(
            max_iter=1,
            learning_rate=0.04,
            max_leaf_nodes=127,
            min_samples_leaf=5,
            l2_regularization=0.1,
            warm_start=True,
            random_state=42
        )

        # warm_start training loop + early stopping
        best_val_rmse = float("inf")
        patience = 50
        no_improve = 0
        best_iter = 1

        for epoch in range(1, 2001):
            model.max_iter = epoch
            model.fit(X_tr, y_tr_l)

            p_vl = model.predict(X_vl)
            rmse_vl = float(np.sqrt(mean_squared_error(y_vl_l, p_vl)))

            if rmse_vl < best_val_rmse:
                best_val_rmse = rmse_vl
                best_iter = epoch
                no_improve = 0
            else:
                no_improve += 1

            if no_improve >= patience:
                break

        # fit l·∫°i best_iter
        model.max_iter = best_iter
        model.fit(X_tr, y_tr_l)

        pred_tr = np.expm1(model.predict(X_tr))
        pred_vl = np.expm1(model.predict(X_vl))
        pred_ts = np.expm1(model.predict(X_ts))

        for name, yt, yp in [
            ("Train", y_tr, pred_tr),
            ("Validation", y_vl, pred_vl),
            ("Test", y_ts, pred_ts),
        ]:
            r2, mae, rmse, mape = calc_metrics(yt, yp)
            results_rows.append([name, r2, mae, rmse, mape])

        # Visualize: TEST
        y_true = y_ts
        y_pred = pred_ts
        resid = y_true - y_pred

        fig_scatter, ax = plt.subplots(figsize=(7, 4))
        ax.scatter(y_true, y_pred, alpha=0.35)
        mn = float(min(y_true.min(), y_pred.min()))
        mx = float(max(y_true.max(), y_pred.max()))
        ax.plot([mn, mx], [mn, mx], linestyle="--")
        ax.set_title("Actual vs Predicted Prices (Test)")
        ax.set_xlabel("Actual Price (VND)")
        ax.set_ylabel("Predicted Price (VND)")
        style_small_ticks(ax)

        fig_resid, ax2 = plt.subplots(figsize=(7, 4))
        ax2.hist(resid, bins=40, alpha=0.8)
        ax2.axvline(0, linestyle="--")
        ax2.set_title("Residuals Distribution (Sai s·ªë - Test)")
        ax2.set_xlabel("Error Amount (VND)")
        ax2.set_ylabel("Count")
        style_small_ticks(ax2)

        # Feature importance: HGB kh√¥ng c√≥ built-in -> d√πng permutation importance (nhanh, l·∫•y top nh·ªè)
        top_k = 12
        try:
            perm = permutation_importance(model, X_vl, y_vl_l, n_repeats=5, random_state=42)
            imp = pd.Series(perm.importances_mean, index=feats).sort_values(ascending=False).head(top_k)[::-1]
            fig_imp, ax3 = plt.subplots(figsize=(7, 4.5))
            ax3.barh(imp.index, imp.values)
            ax3.set_title(f"Top {top_k} Feature Importances (Permutation - HGB)")
            ax3.set_xlabel("Importance (mean decrease)")
            style_small_ticks(ax3)
        except Exception:
            fig_imp = None

        with st.expander("üìå G·ª£i √Ω th√¥ng s·ªë ƒë·ªÉ Test R2 t·ªët h∆°n (HGB)"):
            st.markdown(
                "- `max_leaf_nodes` tƒÉng ‚Üí fit m·∫°nh h∆°n nh∆∞ng d·ªÖ overfit\n"
                "- `min_samples_leaf` tƒÉng ‚Üí m∆∞·ª£t h∆°n, th∆∞·ªùng gi√∫p test t·ªët h∆°n n·∫øu overfit\n"
                "- `learning_rate` nh·ªè + `max_iter` l·ªõn ‚Üí h·ªçc ch·∫≠m, ·ªïn ƒë·ªãnh h∆°n\n"
                f"- Best_iter (theo early stopping) hi·ªán t·∫°i: **{best_iter}**"
            )

    # =========================
    # OUTPUT: TABLE + CAPTIONS
    # =========================
    st.success("‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t!")

    st.subheader("B·∫£ng k·∫øt qu·∫£")
    st.caption("R2: m·ª©c ƒë·ªô m√¥ h√¨nh gi·∫£i th√≠ch bi·∫øn thi√™n gi√° (g·∫ßn 1 l√† t·ªët). MAE: sai s·ªë tuy·ªát ƒë·ªëi trung b√¨nh (VND). RMSE: ph·∫°t sai s·ªë l·ªõn m·∫°nh h∆°n (VND). MAPE: % sai s·ªë trung b√¨nh.")
    res_df = make_results_df(results_rows)
    st.dataframe(res_df, use_container_width=True)

    # =========================
    # VISUALIZE (3 H√ÄNG RI√äNG)
    # =========================
    st.subheader("Visualize (3 ph·∫ßn)")

    st.markdown("### 1) Actual vs Predicted (Test)")
    st.pyplot(fig_scatter, clear_figure=False)

    st.markdown("### 2) Residuals Distribution (Test)")
    st.pyplot(fig_resid, clear_figure=False)

    st.markdown("### 3) Feature Importance")
    if fig_imp is not None:
        st.pyplot(fig_imp, clear_figure=False)
    else:
        st.info("Model n√†y kh√¥ng c√≥ feature importance tr·ª±c ti·∫øp (ho·∫∑c kh√¥ng t√≠nh ƒë∆∞·ª£c trong l·∫ßn ch·∫°y n√†y).")
